{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9408a59",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import bayesnewton\n",
    "import jax\n",
    "import objax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import math   \n",
    "from jax import vmap\n",
    "from scipy.stats import beta\n",
    "\n",
    "\n",
    "import cv2\n",
    "import sys, os\n",
    "sys.path.append('../Utils')\n",
    "import model_utils as mutils\n",
    "import kernels_definitions as kerns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437e0fc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#DATA VARIABLES\n",
    "SYSTEMS_NUM = 2\n",
    "TIMESTEPS_NUM = 50000\n",
    "TRAIN_FRAC = 24  #IF TRAIN_FRAC > 1 THEN IT BECOMES THE LENGTH OF THE TEST SET\n",
    "GRID_PIXELS = 10\n",
    "\n",
    "#OPTIMISATION VARIABLES\n",
    "LR_ADAM = 0.01\n",
    "LR_NEWTON = 0.5\n",
    "ITERS = 10\n",
    "\n",
    "#GP Variables\n",
    "VAR_Y = 0.1\n",
    "LEN_SPACE = 0.5\n",
    "LEN_ALTITUDE = 0.3\n",
    "\n",
    "#PERIODIC KERNEL\n",
    "VAR_PERIOD = 1.5\n",
    "VAR_MATERN = 1\n",
    "LEN_MATERN = 24 /  (TIMESTEPS_NUM / 100) #48\n",
    "LEN_PERIOD = 400 /  (TIMESTEPS_NUM / 100)#24\n",
    "\n",
    "#Want to use a sparse approximation\n",
    "SPARSE = True\n",
    "#Should we optimise the inducing points\n",
    "OPT_Z = False  # will be set to False if SPARSE=SPARSE\n",
    "\n",
    "#use a mean field approximation?\n",
    "MEAN_FIELD = True\n",
    "MINI_BATCH_SIZE = None #none if you don't want them\n",
    "TEST_STATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874b4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime')\n",
    "uk_pv = pd.read_csv('../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "#data_multiple.plot(legend=False)\n",
    "lats = dict(uk_pv.set_index('ss_id')['latitude_noisy'])\n",
    "longs = dict(uk_pv.set_index('ss_id')['longitude_noisy'])\n",
    "data_multiple = data.sample(frac=1, axis=1).iloc[:, :SYSTEMS_NUM][:TIMESTEPS_NUM].reset_index()\n",
    "stacked = mutils.stack_dataframe(data_multiple, lats, longs)\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.columns)].set_index('ss_id_string')['kwp'].values * 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc04a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(stacked[['epoch', 'longitude', 'latitude']])\n",
    "Y = np.array(stacked[['PV']])\n",
    "\n",
    "# convert to easting and northings\n",
    "british_national_grid_coords = convert_bng(X[:, 1], X[:, 2])\n",
    "X = np.vstack([X[:, 0],\n",
    "              np.array(british_national_grid_coords[0]),\n",
    "              np.array(british_national_grid_coords[1])]).T\n",
    "\n",
    "#Create a space-time grid from X and Y\n",
    "t, R, Y = bayesnewton.utils.create_spatiotemporal_grid(X, Y)\n",
    "#SCALING THE t HERE\n",
    "t = t / (TIMESTEPS_NUM / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457fd888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/geopandas/array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Grid of initial inducing points')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train test split for 3 dimensional data\n",
    "t_train, t_test, R_train, R_test, Y_train, Y_test = mutils.train_split_3d(t, R, Y, train_frac = TRAIN_FRAC, split_type = 'Cutoff')\n",
    "Y = Y[:,:,0]\n",
    "\n",
    "#get the mask of the test points\n",
    "test_mask = np.in1d(t.squeeze(), t_test.squeeze())\n",
    "\n",
    "#Scale the data\n",
    "scaled_values = mutils.scale_2d_train_test_data(R, Y, R_train, R_test, Y_train, Y_test )\n",
    "R_scaler, R_scaled, R_train_scaled, R_test_scaled, _, _, _, _ = scaled_values\n",
    "\n",
    "#here get a list of scaled coordinates (frozen because at some point in time)\n",
    "R_scaled_frozen = R_scaled[0]\n",
    "\n",
    "# #Create a grid to perform prediction/interpolation on\n",
    "r1, r2, Rplot = mutils.create_grid_from_coords(R = R_scaled_frozen, t = t, R_scaler = R_scaler, N_pixels = GRID_PIXELS, date_solar = None)\n",
    "\n",
    "z = R_scaled[2, ...]\n",
    "    \n",
    "plt.scatter(*zip(*z[:, :2]), marker='o', s=30, color='red')\n",
    "plt.title('Grid of initial inducing points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2ad582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXED WINDOW OF 5000 train and 24 test, the 5000 train slide forward\n",
    "length_window = 97 * 10\n",
    "max_t = 14000 #len(data_multiple) - length_window - 24\n",
    "iter_step = 50\n",
    "#HERE BUILDING ARRAY OF STARTING ts\n",
    "data_multiple = data_multiple.set_index('datetime')\n",
    "data_multiple.index = pd.to_datetime(data_multiple.index)\n",
    "array_of_indices = data_multiple.reset_index()[(data_multiple.reset_index().datetime.dt.hour > 9) & (data_multiple.reset_index().datetime.dt.hour < 14)].index.values\n",
    "data_multiple = data_multiple.reset_index()\n",
    "range_idx = array_of_indices[10000:max_t:iter_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b8853e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for Gaussian Process\n",
      "NEW ITERATION WITH t: 20248\n",
      "TRAIN SIZE IS: t_train_CV: (970, 1), R_train_scaled_CV: (970, 2, 2), Y_train_CV :(970, 2)\n",
      "TEST SIZE IS: t_test_CV: (24, 1), R_test_scaled_CV: (24, 2, 2), Y_test_CV :(24, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:5373: UserWarning: 'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n",
      "  warnings.warn(\"'kind' argument to argsort is ignored; only 'stable' sorts \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING\n",
      "iter  1, energy: 923.8025\n"
     ]
    },
    {
     "ename": "UnexpectedTracerError",
     "evalue": "Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with shape () and dtype uint32 to escape.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was jit at /Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:244 traced for jit.\n------------------------------\nThe leaked intermediate value was created on line /Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/optimizer/adam.py:62 (__call__). \n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:258 (__call__)\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:248 (jit)\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:184 (__call__)\n/var/folders/ry/13kbd8ws0q1935vwt56pkc8w0000gn/T/ipykernel_88819/636702931.py:57 (train_op)\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/optimizer/adam.py:62 (__call__)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnexpectedTracerError\u001B[0m                     Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, iterations_n \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m t_idx \u001B[38;5;241m==\u001B[39m range_idx[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m---> 72\u001B[0m         \u001B[43mtrain_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m         reduced_train_op(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:258\u001B[0m, in \u001B[0;36mJit.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the compiled version of the function or module.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 258\u001B[0m     output, changes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvc\u001B[38;5;241m.\u001B[39massign(changes)\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "    \u001B[0;31m[... skipping hidden 14 frame]\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:248\u001B[0m, in \u001B[0;36mJit.__init__.<locals>.jit\u001B[0;34m(tensor_list, kwargs, *args)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvc\u001B[38;5;241m.\u001B[39massign(tensor_list)\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvc\u001B[38;5;241m.\u001B[39mtensors()\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvc\u001B[38;5;241m.\u001B[39massign(original_values)\n",
      "File \u001B[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:184\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the the function.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__wrapped__\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36mtrain_op\u001B[0;34m(batch_ind)\u001B[0m\n\u001B[1;32m     55\u001B[0m model\u001B[38;5;241m.\u001B[39minference(lr\u001B[38;5;241m=\u001B[39mLR_NEWTON, batch_ind \u001B[38;5;241m=\u001B[39m batch_ind)  \u001B[38;5;66;03m#perform inference and update variational params\u001B[39;00m\n\u001B[1;32m     56\u001B[0m dE, E \u001B[38;5;241m=\u001B[39m energy()  \u001B[38;5;66;03m# compute energy and its gradients w.r.t. hypers\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m \u001B[43mopt_hypers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mLR_ADAM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdE\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/optimizer/adam.py:62\u001B[0m, in \u001B[0;36mAdam.__call__\u001B[0;34m(self, lr, grads, beta1, beta2)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m beta2 \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     61\u001B[0m     beta2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta2\n\u001B[0;32m---> 62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     63\u001B[0m lr \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m jn\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mvalue)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m g, p, m, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(grads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_vars, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv):\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:6747\u001B[0m, in \u001B[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   6745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, _accepted_binop_types):\n\u001B[1;32m   6746\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[0;32m-> 6747\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbinary_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 5 frame]\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/interpreters/partial_eval.py:1196\u001B[0m, in \u001B[0;36mDynamicJaxprTracer._assert_live\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_live\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1195\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trace\u001B[38;5;241m.\u001B[39mmain\u001B[38;5;241m.\u001B[39mjaxpr_stack:  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m-> 1196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39mescaped_tracer_error(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mUnexpectedTracerError\u001B[0m: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with shape () and dtype uint32 to escape.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was jit at /Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:244 traced for jit.\n------------------------------\nThe leaked intermediate value was created on line /Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/optimizer/adam.py:62 (__call__). \n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:258 (__call__)\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:248 (jit)\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/module.py:184 (__call__)\n/var/folders/ry/13kbd8ws0q1935vwt56pkc8w0000gn/T/ipykernel_88819/636702931.py:57 (train_op)\n/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/objax/optimizer/adam.py:62 (__call__)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(f'Getting results for Gaussian Process')\n",
    "\n",
    "errors = np.zeros((24, 1))\n",
    "NNLs = np.zeros((24, 1))\n",
    "\n",
    "forecast_size = 24\n",
    "for t_idx in range_idx[:3]: \n",
    "    print('NEW ITERATION WITH t:',t_idx)\n",
    "        \n",
    "    t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "    t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "    t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  \n",
    "    print(f'TRAIN SIZE IS: t_train_CV: {t_train_CV.shape}, R_train_scaled_CV: {R_train_scaled_CV.shape}, Y_train_CV :{Y_train_CV.shape}')\n",
    "    print(f'TEST SIZE IS: t_test_CV: {t_test_CV.shape}, R_test_scaled_CV: {R_test_scaled_CV.shape}, Y_test_CV :{Y_test_CV.shape}')\n",
    "\n",
    "    #IF WE ARE IN THE FIRST ITERATION\n",
    "    if t_idx == range_idx[0]:\n",
    "        \n",
    "        kern = kerns.get_periodic_kernel(variance_period = VAR_PERIOD, \n",
    "                                         variance_matern = VAR_MATERN, \n",
    "                                         lengthscale_time_period = LEN_PERIOD, \n",
    "                                         lengthscale_time_matern = LEN_MATERN,\n",
    "                                           lengthscale_space=[LEN_SPACE, LEN_SPACE], #[LEN_SPACE, LEN_SPACE, LEN_ALTITUDE]\n",
    "                                           z=z,\n",
    "                                           sparse=SPARSE,\n",
    "                                           opt_z=OPT_Z,\n",
    "                                           conditional='Full',\n",
    "                                           matern_order = '32',\n",
    "                                           order= 2)\n",
    "\n",
    "        lik = bayesnewton.likelihoods.Beta(scale = 30, fix_scale=False, link='probit')\n",
    "    \n",
    "    model = bayesnewton.models.MarkovVariationalMeanFieldGP(kernel = kern, likelihood = lik, X=t_train_CV, Y=Y_train_CV, R=R_train_scaled_CV)\n",
    "    \n",
    "    #IF WE ARE NOT IN THE FIRST ITERATION, WARM-START THE TRAINING\n",
    "    if t_idx != range_idx[0]:\n",
    "        print('Warm starting the training')\n",
    "        #HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "        for key in model.vars().keys():\n",
    "            if model.vars()[key].shape  == ():\n",
    "                continue\n",
    "            else:\n",
    "                if model.vars()[key].shape[0] == len(t_train_CV):\n",
    "                    shared_var = model.vars()[key] \n",
    "                    init_array = jax.numpy.pad(previous_model.vars()[key][iter_step:], ((0,iter_step), (0,0), (0,0)))\n",
    "                    shared_var.assign(init_array) \n",
    "\n",
    "    \n",
    "    opt_hypers = objax.optimizer.Adam(model.vars())\n",
    "    energy = objax.GradValues(model.energy, model.vars())\n",
    "    \n",
    "    @objax.Function.with_vars(model.vars())\n",
    "    def train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "        dE, E = energy()  # compute energy and its gradients w.r.t. hypers\n",
    "        opt_hypers(LR_ADAM, dE)\n",
    "#     train_op = objax.Jit(train_op)\n",
    "\n",
    "    @objax.Function.with_vars(model.vars())\n",
    "    def reduced_train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "#     reduced_train_op = objax.Jit(reduced_train_op)\n",
    "\n",
    "    print('BEGIN TRAINING')\n",
    "    t0 = time.time()\n",
    "    loss = []\n",
    "    #DOING HALF THE ITERATIONS WHEN UPDATING THE MODEL\n",
    "    iterations_n = ITERS if t_idx == range_idx[0] else int(ITERS/2) \n",
    "    for i in range(1, iterations_n + 1):\n",
    "        if t_idx == range_idx[0]:\n",
    "            train_op(None)\n",
    "        else:\n",
    "            reduced_train_op(None)\n",
    "        loss.append(model.compute_kl().item())\n",
    "        print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "    t1 = time.time()\n",
    "    print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "    \n",
    "    print('Performing the predictions')\n",
    "    #GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "\n",
    "    f_mean, f_var = model.predict(X=t_test_CV, R=R_test_scaled_CV)\n",
    "\n",
    "    #################GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "    f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "    f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "    mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "    posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "    \n",
    "    ##################GET THE ERRORS\n",
    "    error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "    print(f'mae is {error.mean()} \\n')\n",
    "    \n",
    "    errors = np.concatenate((errors, error), axis=1)  \n",
    "    \n",
    "    #################### GET THE NNL\n",
    "    \n",
    "    #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "    N_samples = 1000\n",
    "    #Sample values of f at each point\n",
    "    sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "    alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "    beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "    #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "    observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)\n",
    "    observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "    likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "    NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "    \n",
    "    NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "    #####################\n",
    "    \n",
    "    previous_model = model\n",
    "    \n",
    "    del t_iter, R_iter, Y_iter, t_train_CV, R_train_scaled_CV, Y_train_CV, t_test_CV, R_test_scaled_CV, Y_test_CV\n",
    "    del NNL_hsteps, observed_repeated, beta_sampled, alpha_sampled, sampled_f, error, posterior_mean_ts, posterior_var_ts\n",
    "    del mean_y, var_y, f_mean, f_var, #model\n",
    "    \n",
    "error_evolution = errors[:, 1:].mean(axis=0)\n",
    "MAE_hsteps = errors[:, 1:].mean(axis=1)\n",
    "NNLs_hsteps = NNLs[:, 1:].mean(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################GET THE ERRORS\n",
    "# error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "# print(f'mae is {error.mean()} \\n')\n",
    "\n",
    "# errors = np.concatenate((errors, error), axis=1)  \n",
    "\n",
    "# #################### GET THE NNL\n",
    "\n",
    "# #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "# N_samples = 1000\n",
    "# #Sample values of f at each point\n",
    "# sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "# alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "# beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "# #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "# observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)\n",
    "# observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "# likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "# NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "\n",
    "# NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "# #####################\n",
    "\n",
    "# previous_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3459baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NNLs_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13db7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20384f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MAE_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(NNLs_hsteps).rename(columns ={0:'NNLs_hsteps'} ).to_csv('NNLs_hsteps')\n",
    "pd.DataFrame(error_evolution).rename(columns ={0:'error_evolution'} ).to_csv('error_evolution')\n",
    "pd.DataFrame(MAE_hsteps).rename(columns ={0:'MAE_hsteps'} ).to_csv('MAE_hsteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test_CV) + 50\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "#GET THE ERRORS\n",
    "print(f'testing using the next {len(Y[-len_samples:][test_mask_shortened])} datapoints')\n",
    "error = np.nanmean(abs(np.squeeze(Y[-len_samples:][test_mask_shortened]) - np.squeeze(posterior_mean_ts[test_mask_shortened])), axis=1)\n",
    "print(f'mae is {error}')\n",
    "\n",
    "# errors = np.concatenate((errors, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e555",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test) + 500\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "t1 = time.time()\n",
    "print('prediction time: %2.2f secs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de01e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GET THE PREDICTION INTERVALS AND CALCULATE ERRORS\n",
    "\n",
    "posterior_pos_twostd_rescaled = posterior_mean_ts + 1.96 * np.sqrt(posterior_var_ts)\n",
    "posterior_neg_twostd_rescaled = posterior_mean_ts - 1.96 * np.sqrt(posterior_var_ts)\n",
    "\n",
    "rescaled_Y = (Y ) #* capacities)\n",
    "rescaled_posterior = posterior_mean_ts#) #* capacities\n",
    "\n",
    "#adjust this for the correct quantities\n",
    "mae = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:]) - np.squeeze(rescaled_posterior)))\n",
    "print(f'The MAE is {mae.round(3)}')\n",
    "\n",
    "mae_train = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][~test_mask_shortened]) - np.squeeze(rescaled_posterior[~test_mask_shortened])))\n",
    "print(f'The train MAE is {mae_train.round(3)}')\n",
    "\n",
    "mae_test = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][test_mask_shortened]) - np.squeeze(rescaled_posterior[test_mask_shortened])), axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(mae_test)\n",
    "plt.title('Error as function of forecast distance for Gaussian Process on validation test')\n",
    "plt.xlabel('Number of steps ahead (5min ticks)')\n",
    "plt.ylabel('Average MW error')\n",
    "\n",
    "print(f'The average 2 hours test MAE is {mae_test[:24].mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE THE UNCERTAINTY BOUNDS\n",
    "\n",
    "#Sample values of f at each point\n",
    "sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(10, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "beta_samples = np.random.beta(alpha_sampled, beta_sampled, size=(alpha_sampled.shape[0], alpha_sampled.shape[1], alpha_sampled.shape[2]))\n",
    "lower_bounds_beta_MC = np.quantile(beta_samples, 0.025, axis=0)\n",
    "upper_bounds_beta_MC = np.quantile(beta_samples, 0.975, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27642700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days_index = max(97, int(((len_samples / 3) // 97) * 97)) #number of time intervals to match 5 beginnings of days\n",
    "\n",
    "for i in range(SYSTEMS_NUM):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(f'Prediction for system {i}')\n",
    "    plt.plot(np.arange(len(Y))[-len_samples:], Y_iter[:,i][-len_samples:], \"xk\")\n",
    "    plt.plot(np.arange(len(Y))[-len_samples:], posterior_mean_ts[:,i][-len_samples:], c=\"C0\", lw=2, zorder=2)\n",
    "#     plt.plot(np.arange(len(Y))[-len_samples:], R[-len_samples:, i, 2], 'green', alpha = 0.1)\n",
    "    plt.vlines(t_train[-1], 0, 1, colors='k')\n",
    "\n",
    "#     plt.fill_between(\n",
    "#         np.arange(len(Y))[-len_samples:],\n",
    "#         lower_bounds_beta_MC[:,i],\n",
    "#         upper_bounds_beta_MC[:,i],\n",
    "#         color=\"C1\",\n",
    "#         alpha=0.2)\n",
    "    \n",
    "    plt.xticks(ticks = np.arange(len(Y_iter))[-len_samples:-1:days_index], labels = data_multiple.datetime[-len_samples:-1:days_index].values, size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0617cf",
   "metadata": {},
   "source": [
    "# WARM START FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST INITIALISE A SECOND MODEL\n",
    "#THIS INITIALISATION USES THE SAME KERNEL AND LIKELIHOOD!\n",
    "model2 = bayesnewton.models.MarkovVariationalGP(kernel = kern, likelihood = lik, X=t, Y=Y, R=R_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "for key in model2.vars().keys():\n",
    "    if model2.vars()[key].shape  == ():\n",
    "        continue\n",
    "    else:\n",
    "        if model2.vars()[key].shape[0] == len(t):\n",
    "            shared_var = model2.vars()[key] \n",
    "            init_array = jax.numpy.pad(model.vars()[key], ((0,len(t) - len(t_train)), (0,0), (0,0)))\n",
    "            shared_var.assign(init_array) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02201d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS WHAT IT LOOKS LIKE IN THE MODEL POSTERIOR\n",
    "plt.plot(model2.vars()['(MarkovVariationalGP).posterior_mean'][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "@objax.Function.with_vars(model2.vars())\n",
    "def reduced_train_op(batch_ind = None):\n",
    "    model2.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "reduced_train_op = objax.Jit(reduced_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, 1 + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        if number_of_minibatches > 1:\n",
    "            print(f'Doing minibatch {mini_batch}')\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model2.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8eb0b0",
   "metadata": {},
   "source": [
    "# Compare it to the case without warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST INITIALISE A SECOND MODEL\n",
    "#THIS INITIALISATION USES THE SAME KERNEL AND LIKELIHOOD!\n",
    "kern3 = kerns.get_periodic_kernel(variance=VAR_F,\n",
    "                                           lengthscale_time=LEN_TIME,\n",
    "                                           lengthscale_space=[LEN_SPACE, LEN_SPACE],\n",
    "                                           z=z,\n",
    "                                           sparse=SPARSE,\n",
    "                                           opt_z=OPT_Z,\n",
    "                                           conditional='FIC')\n",
    "\n",
    "lik3 = bayesnewton.likelihoods.Beta(scale = 30, fix_scale=False, link='probit')\n",
    "model3 = bayesnewton.models.MarkovVariationalGP(kernel = kern3, likelihood = lik3, X=t, Y=Y, R=R_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_hypers3 = objax.optimizer.Adam(model3.vars())\n",
    "energy3 = objax.GradValues(model3.energy, model3.vars())\n",
    "\n",
    "@objax.Function.with_vars(model3.vars())\n",
    "def train_op_model3(batch_ind = None):\n",
    "    model3.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "    dE, E = energy3()  # compute energy and its gradients w.r.t. hypers\n",
    "    opt_hypers3(LR_ADAM, dE)\n",
    "train_op_model3 = objax.Jit(train_op_model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bad32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        if number_of_minibatches > 1:\n",
    "            print(f'Doing minibatch {mini_batch}')\n",
    "        train_op_model3(mini_batches_indices[mini_batch])\n",
    "        loss.append(model3.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e923dc",
   "metadata": {},
   "source": [
    "# IMPLEMENT THE CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68662cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5b5238",
   "metadata": {},
   "source": [
    "# TESTING JIT AND MINIBATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adfiheg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f12083",
   "metadata": {},
   "source": [
    "## JIT + no minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = None\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bed763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c78324",
   "metadata": {},
   "source": [
    "## JIT + Time Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4749a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3b175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c2d5b",
   "metadata": {},
   "source": [
    "## no JIT + time minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "#         reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeca908",
   "metadata": {},
   "source": [
    "## no JIT + no minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = None\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49110d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "#         reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5033",
   "metadata": {},
   "source": [
    "## JIT + space minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc03b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a6796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79357a05",
   "metadata": {},
   "source": [
    "## no JIT + space minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9be80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f5cd90d",
   "metadata": {},
   "source": [
    "# INFINITE HORIZON: ONLINE LEARNING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bf0c",
   "metadata": {},
   "source": [
    "## Validation for the model\n",
    "\n",
    "the Kalman filter usually has the following loop:\n",
    "\n",
    "- For i in infinity:\n",
    "    - Get values of t, R (just add one to t and maintain the same R)\n",
    "    - model.predict(X = t, R = R)\n",
    "    - model.some_update_fn(Y = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc0dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_test, var_test = model.predict_y(X = t_test[0:2], R = R_test[0:2])\n",
    "for i in range(2, 50, 2):\n",
    "    mean_test_i, var_test_i = model.predict_y(X = t_test[i:i+5], R = R_test[i:i+5])\n",
    "    mean_test = np.concatenate([mean_test, mean_test_i])\n",
    "    var_test = np.concatenate([var_test, var_test_i])\n",
    "    model.update_posterior()\n",
    "    model.update_variational_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1296b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_mean_ts, posterior_var_ts = model.predict_y(X=t, R=R_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6f995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d190e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.posterior_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693228f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = model.posterior_mean\n",
    "model.X = np.append(t_train, t_test[0])\n",
    "model.Y = np.append(Y_train, Y_test[0])\n",
    "model.R = np.append(R_train_scaled, R_test_scaled[0])\n",
    "model.update_posterior()\n",
    "b = model.posterior_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae831113",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a == b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c201e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.posterior_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_variational_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.filter(1, model.kernel, 0.5, model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6de79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_mean = model.compute_full_pseudo_lik()[0]\n",
    "latent_var = model.compute_full_pseudo_lik()[1]\n",
    "\n",
    "predictive_pseudo = model.likelihood.predict(latent_mean, latent_var, model.mask_pseudo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS METHOD GETS PREDICTIONS DONE \n",
    "\n",
    "# lik = bayesnewton.likelihoods.Gaussian(variance=VAR_Y)\n",
    "# inf = bayesnewton.inference.Taylor\n",
    "# mod = bayesnewton.basemodels.MarkovGP\n",
    "# Mod = bayesnewton.build_model(mod, inf)\n",
    "# model = Mod(kernel=kern, likelihood=lik, X=t_train, Y=Y_train, R=R_train_scaled)\n",
    "\n",
    "# model.inference()\n",
    "# peudo_y = model.compute_full_pseudo_lik()[0]\n",
    "# noise_cov = model.compute_full_pseudo_lik()[1]\n",
    "# model.update_posterior()\n",
    "# log_lik, (filter_mean, filter_cov) = model.filter(model.dt, model.kernel, peudo_y, noise_cov)\n",
    "\n",
    "# filter_mean[:,0,0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}