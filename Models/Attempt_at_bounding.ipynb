{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesnewton\n",
    "import jax\n",
    "import objax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import math   \n",
    "\n",
    "import cv2\n",
    "import sys, os\n",
    "sys.path.append('../Utils')\n",
    "import model_utils as mutils\n",
    "import kernels_definitions as kerns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c004eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA VARIABLES\n",
    "SYSTEMS_NUM = 15 \n",
    "TIMESTEPS_NUM = 300 \n",
    "TRAIN_FRAC = 0.9\n",
    "GRID_PIXELS = 20\n",
    "\n",
    "#OPTIMISATION VARIABLES\n",
    "LR_ADAM = 0.05\n",
    "LR_NEWTON = 0.5\n",
    "ITERS = 3\n",
    "\n",
    "#GP Variables\n",
    "VAR_Y = 0.5\n",
    "VAR_F = 0.5\n",
    "LEN_TIME = 20  # step size = 1 (hour)\n",
    "LEN_SPACE = 1 \n",
    "\n",
    "#Want to use a sparse approximation\n",
    "SPARSE = True\n",
    "#Should we optimise the inducing points\n",
    "OPT_Z = True  # will be set to False if SPARSE=SPARSE\n",
    "\n",
    "#use a mean field approximation?\n",
    "MEAN_FIELD = False\n",
    "MINI_BATCH_SIZE = None #none if you don't want them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34353cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime').drop(columns=['2657', '2828']) #DROPPING FAULTY SYSTEMS\n",
    "uk_pv = pd.read_csv('../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "data_multiple = data.iloc[:, :SYSTEMS_NUM][:TIMESTEPS_NUM]\n",
    "#data_multiple.plot(legend=False)\n",
    "lats = dict(uk_pv.set_index('ss_id')['latitude_rounded'])\n",
    "longs = dict(uk_pv.set_index('ss_id')['longitude_rounded'])\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.columns)].set_index('ss_id_string')['kwp'].values * 1000\n",
    "a = data_multiple.reset_index()\n",
    "stacked = mutils.stack_dataframe(a, lats, longs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d705c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(stacked[['epoch', 'longitude', 'latitude']])\n",
    "Y = np.array(stacked[['PV']])\n",
    "\n",
    "# convert to easting and northings\n",
    "british_national_grid_coords = convert_bng(X[:, 1], X[:, 2])\n",
    "X = np.vstack([X[:, 0],\n",
    "              np.array(british_national_grid_coords[0]),\n",
    "              np.array(british_national_grid_coords[1])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a space-time grid from X and Y\n",
    "t, R, Y = bayesnewton.utils.create_spatiotemporal_grid(X, Y)\n",
    "\n",
    "#train test split for 3 dimensional data\n",
    "t_train, t_test, R_train, R_test, Y_train, Y_test = mutils.train_split_3d(t, R, Y, train_frac = TRAIN_FRAC, split_by_day = False)\n",
    "\n",
    "#get the mask of the test points\n",
    "test_mask = np.in1d(t.squeeze(), t_test.squeeze())\n",
    "\n",
    "#Scale the data\n",
    "scaled_values = mutils.scale_2d_train_test_data(R, Y, R_train, R_test, Y_train, Y_test )\n",
    "R_scaler, R_scaled, R_train_scaled, R_test_scaled, _, _, _, _ = scaled_values\n",
    "\n",
    "#here get a list of scaled coordinates (frozen because at some point in time)\n",
    "R_scaled_frozen = R_scaled[0]\n",
    "\n",
    "# #Create a grid to perform prediction/interpolation on\n",
    "r1, r2, Rplot = mutils.create_grid_from_coords(R = R_scaled_frozen, t = t, R_scaler = R_scaler, N_pixels = GRID_PIXELS)\n",
    "\n",
    "if SPARSE:\n",
    "    z = mutils.create_ind_point_grid(R_scaled_frozen, n_points = None)\n",
    "else:\n",
    "    z = R[0, ...]\n",
    "    \n",
    "plt.scatter(*zip(*z), marker='o', s=30, color='red')\n",
    "plt.title('Grid of initial inducing points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = kerns.get_SpatioTemporal_combined(variance=VAR_F,\n",
    "                                           lengthscale_time=LEN_TIME,\n",
    "                                           lengthscale_space=[LEN_SPACE, LEN_SPACE],\n",
    "                                           z=z,\n",
    "                                           sparse=SPARSE,\n",
    "                                           opt_z=OPT_Z,\n",
    "                                           matern_order = '32',\n",
    "                                           conditional='Full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "lik = bayesnewton.likelihoods.\n",
    "\n",
    "if MEAN_FIELD:\n",
    "    model = bayesnewton.models.MarkovVariationalMeanFieldGP(kernel=kern, likelihood=lik, X=t_train, R=R_train_scaled, Y=Y_train, parallel = True)\n",
    "else:\n",
    "    model = bayesnewton.models.MarkovVariationalGP(kernel=kern, likelihood=lik, X=t_train, R=R_train_scaled, Y=Y_train, parallel = True)\n",
    "\n",
    "opt_hypers = objax.optimizer.Adam(model.vars())\n",
    "energy = objax.GradValues(model.energy, model.vars())\n",
    "\n",
    "@objax.Function.with_vars(model.vars() + opt_hypers.vars())\n",
    "def train_op(batch_ind = None):\n",
    "    model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  # perform inference and update variational params\n",
    "    dE, E = energy()  # compute energy and its gradients w.r.t. hypers\n",
    "    opt_hypers(LR_ADAM, dE)\n",
    "    return E\n",
    "\n",
    "# train_op = objax.Jit(train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1d242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        if number_of_minibatches > 1:\n",
    "            print(f'Doing minibatch {mini_batch}')\n",
    "        loss = train_op(mini_batches_indices[mini_batch])\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[0]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b1182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate posterior predictive distribution via filtering and smoothing at train & test locations:\n",
    "t0 = time.time()\n",
    "print('calculating the posterior predictive distribution ...')\n",
    "posterior_mean, posterior_var = model.predict(X=t, R=Rplot)\n",
    "t1 = time.time()\n",
    "print('prediction time: %2.2f secs' % (t1-t0))\n",
    "\n",
    "t2 = time.time()\n",
    "print('calculating the negative log predictive density ...')\n",
    "nlpd = model.negative_log_predictive_density(X=t_test, R=R_test_scaled, Y=Y_test)\n",
    "t3 = time.time()\n",
    "print('nlpd calculation time: %2.2f secs' % (t3-t2))\n",
    "print('nlpd: %2.3f' % nlpd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f458980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_opt = model.kernel.z.value\n",
    "# mu = Y_scaler.inverse_transform(posterior_mean.flatten()[:, np.newaxis]).reshape(-1, GRID_PIXELS, GRID_PIXELS)\n",
    "mu = posterior_mean.reshape(TIMESTEPS_NUM, GRID_PIXELS, GRID_PIXELS)\n",
    "Y = Y[:,:,0]\n",
    "\n",
    "#get lat-lon coordinates\n",
    "grid_coord = R_scaler.inverse_transform(np.array(np.c_[r1,r2]))\n",
    "longitude_grid, latitude_grid =  convert_lonlat(grid_coord[:, 0], grid_coord[:, 1])\n",
    "longitude_sys_train, latitude_sys_train = convert_lonlat(R_train[:,:,0][0], R_train[:,:,1][0])\n",
    "longitude_z, latitude_z = convert_lonlat(R_scaler.inverse_transform(z_opt)[:,0], R_scaler.inverse_transform(z_opt)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea771b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_result = False\n",
    "# del model, kern, Rplot  # , var\n",
    "\n",
    "print('plotting ...')\n",
    "cmap = cm.viridis\n",
    "vmin = np.nanpercentile(Y, 1)\n",
    "vmax = np.nanpercentile(Y, 99)\n",
    "#get the labels for the dates\n",
    "dates = pd.to_datetime(a.datetime).dt.date\n",
    "days_index = max(97, int(((len(t) / 5) // 97) * 97)) #number of time intervals to match 5 beginnings of days\n",
    "\n",
    "for time_step in range(t.shape[0])[:50]:\n",
    "    f, (a0, a1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [20, 1]})\n",
    "    f.set_figheight(8)\n",
    "    # f.set_figwidth(8)\n",
    "    im = a0.imshow(mu[time_step], cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                   extent=[longitude_grid[0], longitude_grid[-1], latitude_grid[0], latitude_grid[-1]], origin='lower')\n",
    "    a0.scatter(longitude_sys_train, latitude_sys_train, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "               c=np.squeeze(Y[time_step]), s=50, edgecolors='black')\n",
    "    plt.colorbar(im, fraction=0.0348, pad=0.03, aspect=30, ax=a0)\n",
    "    if SPARSE:\n",
    "        a0.scatter(longitude_z, latitude_z, c='r', s=20, alpha=0.5)  # plot inducing inputs\n",
    "    a0.set_xlim(longitude_grid[0], longitude_grid[-1])\n",
    "    a0.set_ylim(latitude_grid[0], latitude_grid[-1])\n",
    "    a0.set_title(f'PVE at {a.datetime.unique()[time_step]}')\n",
    "    a0.set_ylabel('Latitude')\n",
    "    a0.set_xlabel('Longitude')\n",
    "    a1.vlines(t[time_step].item(), -1, 1, 'r')\n",
    "    a1.set_xlabel('time (days)')\n",
    "    a1.set_xlim(t[0], t[-1])\n",
    "    \n",
    "    a1.set_xticks(np.asarray(t[1:-1:days_index ][:,0].tolist()), \n",
    "                  labels = dates[0:-1:days_index].values,\n",
    "                     fontsize = 10)\n",
    "    plt.show()\n",
    "    plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "\n",
    "posterior_mean_ts, posterior_var_ts = model.predict(X=t, R=R_scaled)\n",
    "# posterior_mean_rescaled = Y_scaler.inverse_transform(posterior_mean_ts)\n",
    "posterior_pos_twostd_rescaled = posterior_mean_ts + 1.96 * np.sqrt(posterior_var_ts)\n",
    "posterior_neg_twostd_rescaled = posterior_mean_ts - 1.96 * np.sqrt(posterior_var_ts)\n",
    "\n",
    "rescaled_Y = (Y * capacities)\n",
    "rescaled_posterior = posterior_mean_ts * capacities\n",
    "\n",
    "#adjust this for the correct quantities\n",
    "rmse = np.sqrt(np.nanmean((np.squeeze(rescaled_Y) - np.squeeze(rescaled_posterior))**2))\n",
    "print(f'The RMSE is {rmse.round(3)}')\n",
    "\n",
    "rmse_train = np.sqrt(np.nanmean((np.squeeze(rescaled_Y[~test_mask]) - np.squeeze(rescaled_posterior[~test_mask]))**2))\n",
    "print(f'The train RMSE is {rmse_train.round(3)}')\n",
    "\n",
    "rmse_test = np.sqrt(np.nanmean((np.squeeze(rescaled_Y[test_mask]) - np.squeeze(rescaled_posterior[test_mask]))**2))\n",
    "print(f'The test RMSE is {rmse_test.round(3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4e41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(SYSTEMS_NUM):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(f'Prediction for system {i}')\n",
    "    plt.plot(np.arange(len(Y)), Y[:,i], \"xk\")\n",
    "    plt.plot(np.arange(len(Y)), posterior_mean_ts[:,i], c=\"C0\", lw=2, zorder=2)\n",
    "    plt.fill_between(\n",
    "        np.arange(len(Y)),\n",
    "        posterior_neg_twostd_rescaled[:,i],\n",
    "        posterior_pos_twostd_rescaled[:,i],\n",
    "        color=\"C0\",\n",
    "        alpha=0.2)\n",
    "    plt.xticks(ticks = np.arange(len(Y))[0:-1:days_index], labels = a.datetime[0:-1:days_index].values, size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf689f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
