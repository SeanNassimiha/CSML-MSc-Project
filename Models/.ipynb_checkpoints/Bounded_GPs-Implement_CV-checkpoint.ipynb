{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9408a59",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import bayesnewton\n",
    "import jax\n",
    "import objax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import math   \n",
    "from jax import vmap\n",
    "from scipy.stats import beta\n",
    "import wandb\n",
    "\n",
    "import cv2\n",
    "import sys, os\n",
    "sys.path.append('../Utils')\n",
    "import model_utils as mutils\n",
    "import kernels_definitions as kerns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "437e0fc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#DATA VARIABLES\n",
    "SYSTEMS_NUM = 100\n",
    "TIMESTEPS_NUM = 50000\n",
    "TRAIN_FRAC = 24  #IF TRAIN_FRAC > 1 THEN IT BECOMES THE LENGTH OF THE TEST SET\n",
    "GRID_PIXELS = 10\n",
    "\n",
    "#OPTIMISATION VARIABLES\n",
    "LR_ADAM = 0.01\n",
    "LR_NEWTON = 0.5\n",
    "ITERS = 100\n",
    "\n",
    "#GP Variables\n",
    "BETA_SCALE = 100\n",
    "LEN_SPACE = 0.3\n",
    "LEN_ALTITUDE = 0.3\n",
    "\n",
    "#PERIODIC KERNEL\n",
    "VAR_PERIOD = 0.8\n",
    "VAR_MATERN = 0.8\n",
    "LEN_MATERN = 24 /  (TIMESTEPS_NUM / 100) #48\n",
    "LEN_PERIOD = 400 /  (TIMESTEPS_NUM / 100)#24\n",
    "\n",
    "#Want to use a sparse approximation\n",
    "SPARSE = True\n",
    "#Should we optimise the inducing points\n",
    "OPT_Z = False  # will be set to False if SPARSE=SPARSE\n",
    "\n",
    "#use a mean field approximation?\n",
    "MEAN_FIELD = False\n",
    "MINI_BATCH_SIZE = None #none if you don't want them\n",
    "TEST_STATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcaa89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = dict (\n",
    "\n",
    "#                 SYSTEMS_NUM = 100,\n",
    "#                 TIMESTEPS_NUM = 50000,\n",
    "#                 TRAIN_FRAC = 24 , #IF TRAIN_FRAC > 1 THEN IT BECOMES THE LENGTH OF THE TEST SET\n",
    "#                 GRID_PIXELS = 10,\n",
    "\n",
    "#                 #OPTIMISATION VARIABLES\n",
    "#                 LR_ADAM = 0.01,\n",
    "#                 LR_NEWTON = 0.5,\n",
    "#                 ITERS = 3,\n",
    "\n",
    "#                 #GP Variables\n",
    "#                 VAR_Y = 0.1,\n",
    "#                 LEN_SPACE = 0.5,\n",
    "#                 LEN_ALTITUDE = 0.3,\n",
    "\n",
    "#                 #PERIODIC KERNEL\n",
    "#                 VAR_PERIOD = 1.5,\n",
    "#                 VAR_MATERN = 1,\n",
    "#                 LEN_MATERN = 24 /  (TIMESTEPS_NUM / 100),   \n",
    "#                 LEN_PERIOD = 400 /  (TIMESTEPS_NUM / 100),  \n",
    "\n",
    "#                 #Want to use a sparse approximation\n",
    "#                 SPARSE = True,\n",
    "#                 #Should we optimise the inducing points\n",
    "#                 OPT_Z = False,  # will be set to False if SPARSE=SPARSE\n",
    "\n",
    "#                 #use a mean field approximation?\n",
    "#                 MEAN_FIELD = True,\n",
    "#                 MINI_BATCH_SIZE = None, #none if you don't want them\n",
    "#                 TEST_STATIONS = 10,\n",
    "\n",
    "# )\n",
    "\n",
    "# wandb.init(project=\"Nowcasting\", entity=\"snassimiha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "874b4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime')\n",
    "uk_pv = pd.read_csv('../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "#data_multiple.plot(legend=False)\n",
    "lats = dict(uk_pv.set_index('ss_id')['latitude_noisy'])\n",
    "longs = dict(uk_pv.set_index('ss_id')['longitude_noisy'])\n",
    "data_multiple = data.sample(frac=1, axis=1).iloc[:, :SYSTEMS_NUM][:TIMESTEPS_NUM].reset_index()\n",
    "stacked = mutils.stack_dataframe(data_multiple, lats, longs)\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.columns)].set_index('ss_id_string')['kwp'].values * 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc04a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(stacked[['epoch', 'longitude', 'latitude']])\n",
    "Y = np.array(stacked[['PV']])\n",
    "\n",
    "# convert to easting and northings\n",
    "british_national_grid_coords = convert_bng(X[:, 1], X[:, 2])\n",
    "X = np.vstack([X[:, 0],\n",
    "              np.array(british_national_grid_coords[0]),\n",
    "              np.array(british_national_grid_coords[1])]).T\n",
    "\n",
    "#Create a space-time grid from X and Y\n",
    "t, R, Y = bayesnewton.utils.create_spatiotemporal_grid(X, Y)\n",
    "#SCALING THE t HERE\n",
    "t = t / (TIMESTEPS_NUM / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "457fd888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/geopandas/array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Grid of initial inducing points')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train test split for 3 dimensional data\n",
    "t_train, t_test, R_train, R_test, Y_train, Y_test = mutils.train_split_3d(t, R, Y, train_frac = TRAIN_FRAC, split_type = 'Cutoff')\n",
    "Y = Y[:,:,0]\n",
    "\n",
    "#get the mask of the test points\n",
    "test_mask = np.in1d(t.squeeze(), t_test.squeeze())\n",
    "\n",
    "#Scale the data\n",
    "scaled_values = mutils.scale_2d_train_test_data(R, Y, R_train, R_test, Y_train, Y_test )\n",
    "R_scaler, R_scaled, R_train_scaled, R_test_scaled, _, _, _, _ = scaled_values\n",
    "\n",
    "#here get a list of scaled coordinates (frozen because at some point in time)\n",
    "R_scaled_frozen = R_scaled[0]\n",
    "\n",
    "# #Create a grid to perform prediction/interpolation on\n",
    "r1, r2, Rplot = mutils.create_grid_from_coords(R = R_scaled_frozen, t = t, R_scaler = R_scaler, N_pixels = GRID_PIXELS, date_solar = None)\n",
    "\n",
    "# z = R_scaled[2, ...]\n",
    "z =  R_scaled[2, ::5]\n",
    "    \n",
    "plt.scatter(*zip(*z[:, :2]), marker='o', s=30, color='red')\n",
    "plt.title('Grid of initial inducing points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2ad582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXED WINDOW OF 5000 train and 24 test, the 5000 train slide forward\n",
    "length_window = 97 * 100\n",
    "max_t = 14000 #len(data_multiple) - length_window - 24\n",
    "iter_step = 50\n",
    "#HERE BUILDING ARRAY OF STARTING ts\n",
    "data_multiple = data_multiple.set_index('datetime')\n",
    "data_multiple.index = pd.to_datetime(data_multiple.index)\n",
    "array_of_indices = data_multiple.reset_index()[(data_multiple.reset_index().datetime.dt.hour > 9) & (data_multiple.reset_index().datetime.dt.hour < 14)].index.values\n",
    "data_multiple = data_multiple.reset_index()\n",
    "# range_idx = array_of_indices[10000:max_t:iter_step]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "759066f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range_idx = np.array([20248, 20347, 20446, 20545, 20644, 20743, 20842, 20941, 21040,\n",
    "#        21139, 21238, 21337, 21436, 21535, 21634, 21782, 21881, 21980,\n",
    "#        22079, 22178, 22277, 22376, 22475, 22574, 22673, 22772, 22871,\n",
    "#        22970, 23069, 23168, 23267, 23366, 23465, 23564, 23663, 23762,\n",
    "#        23861, 23960, 24059, 24207, 24306, 24405, 24504, 24603, 24702,\n",
    "#        24801, 24900, 24999, 25098, 25197, 25296, 25395, 25494, 25593,\n",
    "#        25692, 25791, 25890, 25989, 26088, 26187, 26335, 26434, 26533,\n",
    "#        26632, 26731, 26830, 26929, 27028, 27127, 27226, 27325, 27424,\n",
    "#        27523, 27622, 27721, 27820, 27919, 28018, 28166, 28241])\n",
    "\n",
    "range_idx = np.array([26830])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7b8853e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for Gaussian Process\n",
      "NEW ITERATION WITH t: 26830\n",
      "TRAIN SIZE IS: t_train_CV: (9700, 1), R_train_scaled_CV: (9700, 100, 2), Y_train_CV :(9700, 100)\n",
      "TEST SIZE IS: t_test_CV: (24, 1), R_test_scaled_CV: (24, 100, 2), Y_test_CV :(24, 100)\n",
      "BEGIN TRAINING\n",
      "iter  1, energy: 432490.6041\n",
      "iter  2, energy: 268739.2345\n",
      "iter  3, energy: 181662.9548\n",
      "iter  4, energy: 139833.6687\n",
      "iter  5, energy: 123400.4910\n",
      "iter  6, energy: 120681.6591\n",
      "iter  7, energy: 124958.3681\n",
      "iter  8, energy: 131329.8517\n",
      "iter  9, energy: 136443.4858\n",
      "iter 10, energy: 139020.4611\n",
      "iter 11, energy: 140147.7098\n",
      "iter 12, energy: 141127.1675\n",
      "iter 13, energy: 142604.0545\n",
      "iter 14, energy: 144183.6402\n",
      "iter 15, energy: 144603.8508\n",
      "iter 16, energy: 143520.2053\n",
      "iter 17, energy: 142640.9823\n",
      "iter 18, energy: 140838.9045\n",
      "iter 19, energy: 138970.9811\n",
      "iter 20, energy: 136917.9604\n",
      "iter 21, energy: 139768.0179\n",
      "iter 22, energy: 134608.7395\n",
      "iter 23, energy: 131957.6413\n",
      "iter 24, energy: 129838.9326\n",
      "iter 25, energy: 128080.6835\n",
      "iter 26, energy: 126544.9217\n",
      "iter 27, energy: 125092.1764\n",
      "iter 28, energy: 123555.2684\n",
      "iter 29, energy: 122209.7188\n",
      "iter 30, energy: 120983.4707\n",
      "iter 31, energy: 119795.1268\n",
      "iter 32, energy: 118636.2285\n",
      "iter 33, energy: 117604.9836\n",
      "iter 34, energy: 116581.1923\n",
      "iter 35, energy: 115720.2953\n",
      "iter 36, energy: 114838.6708\n",
      "iter 37, energy: 114055.0288\n",
      "iter 38, energy: 113280.6664\n",
      "iter 39, energy: 112426.6230\n",
      "iter 40, energy: 111751.5072\n",
      "iter 41, energy: 111025.0711\n",
      "iter 42, energy: 110373.2156\n",
      "iter 43, energy: 109724.1893\n",
      "iter 44, energy: 109091.1557\n",
      "iter 45, energy: 108434.0638\n",
      "iter 46, energy: 107795.4465\n",
      "iter 47, energy: 107176.4394\n",
      "iter 48, energy: 106551.8665\n",
      "iter 49, energy: 105942.9522\n",
      "iter 50, energy: 105349.1093\n",
      "iter 51, energy: 104767.0662\n",
      "iter 52, energy: 104208.1865\n",
      "iter 53, energy: 103660.1992\n",
      "iter 54, energy: 103117.6199\n",
      "iter 55, energy: 102579.7385\n",
      "iter 56, energy: 102057.8463\n",
      "iter 57, energy: 101553.0907\n",
      "iter 58, energy: 101054.0644\n",
      "iter 59, energy: 100574.4823\n",
      "iter 60, energy: 100104.5694\n",
      "iter 61, energy: 99637.7686\n",
      "iter 62, energy: 99164.5421\n",
      "iter 63, energy: 98704.6144\n",
      "iter 64, energy: 98251.1171\n",
      "iter 65, energy: 97801.6036\n",
      "iter 66, energy: 97351.9086\n",
      "iter 67, energy: 96891.6940\n",
      "iter 68, energy: 96436.0230\n",
      "iter 69, energy: 95954.6696\n",
      "iter 70, energy: 95486.1923\n",
      "iter 71, energy: 94990.2310\n",
      "iter 72, energy: 94509.1531\n",
      "iter 73, energy: 93996.4267\n",
      "iter 74, energy: 93523.1914\n",
      "iter 75, energy: 93037.8166\n",
      "iter 76, energy: 92608.1546\n",
      "iter 77, energy: 92161.0561\n",
      "iter 78, energy: 91759.1303\n",
      "iter 79, energy: 91346.6513\n",
      "iter 80, energy: 90988.2669\n",
      "iter 81, energy: 90562.0515\n",
      "iter 82, energy: 90160.8839\n",
      "iter 83, energy: 89733.2479\n",
      "iter 84, energy: 89308.8229\n",
      "iter 85, energy: 88877.6342\n",
      "iter 86, energy: 88455.3515\n",
      "iter 87, energy: 88033.4112\n",
      "iter 88, energy: 87618.1866\n",
      "iter 89, energy: 87206.9186\n",
      "iter 90, energy: 86804.3253\n",
      "iter 91, energy: 86411.9746\n",
      "iter 92, energy: 86028.5246\n",
      "iter 93, energy: 85653.7232\n",
      "iter 94, energy: 85281.2236\n",
      "iter 95, energy: 84914.7738\n",
      "iter 96, energy: 84554.1266\n",
      "iter 97, energy: 84209.5933\n",
      "iter 98, energy: 83854.3442\n",
      "iter 99, energy: 83504.5310\n",
      "iter 100, energy: 83156.8790\n",
      "optimisation time: 15837.80 secs\n",
      "Performing the predictions\n",
      "mae is 0.05086958810635369 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(f'Getting results for Gaussian Process')\n",
    "\n",
    "errors = np.zeros((24, 1))\n",
    "NNLs = np.zeros((24, 1))\n",
    "\n",
    "forecast_size = 24\n",
    "for t_idx in range_idx: \n",
    "    print('NEW ITERATION WITH t:',t_idx)\n",
    "        \n",
    "    t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "    t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "    t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  \n",
    "    print(f'TRAIN SIZE IS: t_train_CV: {t_train_CV.shape}, R_train_scaled_CV: {R_train_scaled_CV.shape}, Y_train_CV :{Y_train_CV.shape}')\n",
    "    print(f'TEST SIZE IS: t_test_CV: {t_test_CV.shape}, R_test_scaled_CV: {R_test_scaled_CV.shape}, Y_test_CV :{Y_test_CV.shape}')\n",
    "\n",
    "    #IF WE ARE IN THE FIRST ITERATION\n",
    "    if t_idx == range_idx[0]:\n",
    "        \n",
    "        kern = kerns.get_periodic_kernel(variance_period = VAR_PERIOD, \n",
    "                                         variance_matern = VAR_MATERN, \n",
    "                                         lengthscale_time_period = LEN_PERIOD, \n",
    "                                         lengthscale_time_matern = LEN_MATERN,\n",
    "                                           lengthscale_space=[LEN_SPACE, LEN_SPACE], #[LEN_SPACE, LEN_SPACE, LEN_ALTITUDE]\n",
    "                                           z=z,\n",
    "                                           sparse=SPARSE,\n",
    "                                           opt_z=OPT_Z,\n",
    "                                           conditional='Full',\n",
    "                                           matern_order = '32',\n",
    "                                           order= 2)\n",
    "\n",
    "        lik = bayesnewton.likelihoods.Beta(scale = BETA_SCALE, fix_scale=False, link='probit')\n",
    "    \n",
    "    model = bayesnewton.models.MarkovVariationalMeanFieldGP(kernel = kern, likelihood = lik, X=t_train_CV, Y=Y_train_CV, R=R_train_scaled_CV)\n",
    "    \n",
    "    #IF WE ARE NOT IN THE FIRST ITERATION, WARM-START THE TRAINING\n",
    "    if t_idx != range_idx[0]:\n",
    "        print('Warm starting the training')\n",
    "        #HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "        for key in model.vars().keys():\n",
    "            if model.vars()[key].shape  == ():\n",
    "                continue\n",
    "            else:\n",
    "                if model.vars()[key].shape[0] == len(t_train_CV):\n",
    "                    shared_var = model.vars()[key] \n",
    "                    init_array = jax.numpy.pad(previous_model.vars()[key][iter_step:], ((0,iter_step), (0,0), (0,0)))\n",
    "                    shared_var.assign(init_array) \n",
    "\n",
    "    \n",
    "    opt_hypers = objax.optimizer.Adam(model.vars())\n",
    "    energy = objax.GradValues(model.energy, model.vars())\n",
    "    \n",
    "    @objax.Function.with_vars(model.vars() + opt_hypers.vars())\n",
    "    def train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "        dE, E = energy()  # compute energy and its gradients w.r.t. hypers\n",
    "        opt_hypers(LR_ADAM, dE)\n",
    "    train_op = objax.Jit(train_op)\n",
    "\n",
    "    @objax.Function.with_vars(model.vars())\n",
    "    def reduced_train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "    reduced_train_op = objax.Jit(reduced_train_op)\n",
    "\n",
    "    print('BEGIN TRAINING')\n",
    "    t0 = time.time()\n",
    "    loss = []\n",
    "    #DOING HALF THE ITERATIONS WHEN UPDATING THE MODEL\n",
    "    iterations_n = ITERS if t_idx == range_idx[0] else int(ITERS/2) \n",
    "    for i in range(1, iterations_n + 1):\n",
    "        if t_idx == range_idx[0]:\n",
    "            train_op(None)\n",
    "        else:\n",
    "            reduced_train_op(None)\n",
    "        loss.append(model.compute_kl().item())\n",
    "        print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "    t1 = time.time()\n",
    "    print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "    \n",
    "    print('Performing the predictions')\n",
    "    #GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "\n",
    "    f_mean, f_var = model.predict(X=t_test_CV, R=R_test_scaled_CV)\n",
    "\n",
    "    #################GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "    f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "    f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "    mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "    posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "    \n",
    "    ##################GET THE ERRORS\n",
    "    error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "    print(f'mae is {error.mean()} \\n')\n",
    "    \n",
    "    errors = np.concatenate((errors, error), axis=1)  \n",
    "    \n",
    "    #################### GET THE NNL\n",
    "    \n",
    "    #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "    N_samples = 1000\n",
    "    #Sample values of f at each point\n",
    "    sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "    alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "    beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "    #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "    observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)\n",
    "    observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "    likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "    NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "    \n",
    "    NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "    #####################\n",
    "    \n",
    "    previous_model = model\n",
    "    \n",
    "    del t_iter, R_iter, Y_iter, t_train_CV, R_train_scaled_CV, Y_train_CV, t_test_CV, R_test_scaled_CV, Y_test_CV\n",
    "    del NNL_hsteps, observed_repeated, beta_sampled, alpha_sampled, sampled_f, error, posterior_mean_ts, posterior_var_ts\n",
    "    del mean_y, var_y, f_mean, f_var, #model\n",
    "    \n",
    "error_evolution = errors[:, 1:].mean(axis=0)\n",
    "MAE_hsteps = errors[:, 1:].mean(axis=1)\n",
    "NNLs_hsteps = np.quantile(NNLs[:, 1:], 0.5, axis=1)\n",
    "NNLs_hsteps_upper = np.quantile(NNLs[:, 1:], 0.975,  axis=1)\n",
    "NNLs_hsteps_lower = np.quantile(NNLs[:, 1:], 0.025,  axis=1)\n",
    "\n",
    "\n",
    "error_evolution = pd.DataFrame(error_evolution).rename(columns ={0:'error_evolution'} )\n",
    "MAE_hsteps = pd.DataFrame(MAE_hsteps).rename(columns ={0:'MAE_hsteps'} )\n",
    "NNLs_hsteps = pd.DataFrame(NNLs_hsteps).rename(columns ={0:'NNLs_hsteps'} )\n",
    "\n",
    "# wandb.log({\"error_evolution\": error_evolution,  \"MAE_hsteps\": MAE_hsteps, \"NNLs_hsteps\": NNLs_hsteps})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS TO PLOT THE PREDICTIONS FROM THE MODEL ITERATION\n",
    "\n",
    "t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  \n",
    "\n",
    "f_mean, f_var = model.predict(X=t_iter[-500:], R=R_iter[-500:])\n",
    "\n",
    "#################GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eba43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_mode(alpha, beta):\n",
    "    '''\n",
    "    Calcualte the mode of the beta distribution given values of alpha and beta\n",
    "    \n",
    "    Use:\n",
    "    (ð‘Žâˆ’1)/(ð‘Ž+ð‘âˆ’2)=ð‘šð‘œð‘‘ð‘’\n",
    "    '''\n",
    "    \n",
    "    return (alpha - 1) / (alpha + beta - 2)\n",
    "\n",
    "\n",
    "#Sample values of f at each point\n",
    "sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(500, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "beta_samples = np.random.beta(alpha_sampled, beta_sampled, size=(alpha_sampled.shape[0], alpha_sampled.shape[1], alpha_sampled.shape[2]))\n",
    "lower_bounds_beta_MC = np.quantile(beta_samples, 0.01, axis=0)\n",
    "upper_bounds_beta_MC = np.quantile(beta_samples, 0.99, axis=0)\n",
    "median_MC = np.quantile(beta_samples, 0.5, axis=0)\n",
    "\n",
    "bounds_90_beta_MC = np.quantile(beta_samples, 0.90, axis=0)\n",
    "bounds_10_beta_MC = np.quantile(beta_samples, 0.10, axis=0)\n",
    "\n",
    "mode_beta_MC = beta_mode(alpha_sampled, beta_sampled).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8cb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = 1000\n",
    "\n",
    "for i in range(SYSTEMS_NUM)[:10]:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(f'Prediction for system {i}')\n",
    "\n",
    "    plt.plot(np.arange(len(Y_iter[-history:])), Y_iter[-history:,i], \"xk\", label='Ground truth')\n",
    "    plt.plot(np.arange(len(Y_iter[-history:])), posterior_mean_ts[-history:,i], c=\"C0\", lw=2, zorder=2, label='posterior mean')\n",
    "    plt.plot(np.arange(len(Y_iter[-history:])), median_MC[-history:,i], c=\"red\", lw=2, zorder=2, alpha = 0.5, label='sampled posterior median')\n",
    "#     plt.plot(np.arange(len(Y_iter[-history:])), mode_beta_MC[-history:,i], c=\"orange\", lw=2, zorder=2, alpha = 0.5, linestyle='-.', label='sampled posterior mode')\n",
    "\n",
    "    \n",
    "    plt.vlines(np.arange(len(Y_iter[-history:]))[-24], 0, max(Y_iter[-history:,i].max().item(), upper_bounds_beta_MC[-history:,i].max().item())+0.1, colors='k')\n",
    "\n",
    "    plt.fill_between(\n",
    "        np.arange(len(Y_iter[-history:])),\n",
    "        lower_bounds_beta_MC[-history:,i],\n",
    "        upper_bounds_beta_MC[-history:,i],\n",
    "        color=\"C1\",\n",
    "        alpha=0.2,\n",
    "        label = '1-99 quantiles')\n",
    "    \n",
    "    plt.fill_between(\n",
    "        np.arange(len(Y_iter[-history:])),\n",
    "        bounds_10_beta_MC[-history:,i],\n",
    "        bounds_90_beta_MC[-history:,i],\n",
    "        color=\"green\",\n",
    "        alpha=0.4,\n",
    "        label = '10-90 quantiles')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('time-step')\n",
    "    plt.ylabel('PVE Capacity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################GET THE ERRORS\n",
    "# error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "# print(f'mae is {error.mean()} \\n')\n",
    "\n",
    "# errors = np.concatenate((errors, error), axis=1)  \n",
    "\n",
    "# #################### GET THE NNL\n",
    "\n",
    "# #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "# N_samples = 1000\n",
    "# #Sample values of f at each point\n",
    "# sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "# alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "# beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "# #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "# observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)\n",
    "# observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "# likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "# NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "\n",
    "# NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "# #####################\n",
    "\n",
    "# previous_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3459baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NNLs_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13db7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20384f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MAE_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(NNLs_hsteps).rename(columns ={0:'NNLs_hsteps'} ).to_csv('NNLs_hsteps')\n",
    "pd.DataFrame(error_evolution).rename(columns ={0:'error_evolution'} ).to_csv('error_evolution')\n",
    "pd.DataFrame(MAE_hsteps).rename(columns ={0:'MAE_hsteps'} ).to_csv('MAE_hsteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test_CV) + 50\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "#GET THE ERRORS\n",
    "print(f'testing using the next {len(Y[-len_samples:][test_mask_shortened])} datapoints')\n",
    "error = np.nanmean(abs(np.squeeze(Y[-len_samples:][test_mask_shortened]) - np.squeeze(posterior_mean_ts[test_mask_shortened])), axis=1)\n",
    "print(f'mae is {error}')\n",
    "\n",
    "# errors = np.concatenate((errors, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e555",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test) + 500\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "t1 = time.time()\n",
    "print('prediction time: %2.2f secs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de01e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GET THE PREDICTION INTERVALS AND CALCULATE ERRORS\n",
    "\n",
    "posterior_pos_twostd_rescaled = posterior_mean_ts + 1.96 * np.sqrt(posterior_var_ts)\n",
    "posterior_neg_twostd_rescaled = posterior_mean_ts - 1.96 * np.sqrt(posterior_var_ts)\n",
    "\n",
    "rescaled_Y = (Y ) #* capacities)\n",
    "rescaled_posterior = posterior_mean_ts#) #* capacities\n",
    "\n",
    "#adjust this for the correct quantities\n",
    "mae = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:]) - np.squeeze(rescaled_posterior)))\n",
    "print(f'The MAE is {mae.round(3)}')\n",
    "\n",
    "mae_train = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][~test_mask_shortened]) - np.squeeze(rescaled_posterior[~test_mask_shortened])))\n",
    "print(f'The train MAE is {mae_train.round(3)}')\n",
    "\n",
    "mae_test = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][test_mask_shortened]) - np.squeeze(rescaled_posterior[test_mask_shortened])), axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(mae_test)\n",
    "plt.title('Error as function of forecast distance for Gaussian Process on validation test')\n",
    "plt.xlabel('Number of steps ahead (5min ticks)')\n",
    "plt.ylabel('Average MW error')\n",
    "\n",
    "print(f'The average 2 hours test MAE is {mae_test[:24].mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8eb0b0",
   "metadata": {},
   "source": [
    "# Compare it to the case without warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST INITIALISE A SECOND MODEL\n",
    "#THIS INITIALISATION USES THE SAME KERNEL AND LIKELIHOOD!\n",
    "kern3 = kerns.get_periodic_kernel(variance=VAR_F,\n",
    "                                           lengthscale_time=LEN_TIME,\n",
    "                                           lengthscale_space=[LEN_SPACE, LEN_SPACE],\n",
    "                                           z=z,\n",
    "                                           sparse=SPARSE,\n",
    "                                           opt_z=OPT_Z,\n",
    "                                           conditional='FIC')\n",
    "\n",
    "lik3 = bayesnewton.likelihoods.Beta(scale = 30, fix_scale=False, link='probit')\n",
    "model3 = bayesnewton.models.MarkovVariationalGP(kernel = kern3, likelihood = lik3, X=t, Y=Y, R=R_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_hypers3 = objax.optimizer.Adam(model3.vars())\n",
    "energy3 = objax.GradValues(model3.energy, model3.vars())\n",
    "\n",
    "@objax.Function.with_vars(model3.vars())\n",
    "def train_op_model3(batch_ind = None):\n",
    "    model3.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "    dE, E = energy3()  # compute energy and its gradients w.r.t. hypers\n",
    "    opt_hypers3(LR_ADAM, dE)\n",
    "train_op_model3 = objax.Jit(train_op_model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bad32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        if number_of_minibatches > 1:\n",
    "            print(f'Doing minibatch {mini_batch}')\n",
    "        train_op_model3(mini_batches_indices[mini_batch])\n",
    "        loss.append(model3.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e923dc",
   "metadata": {},
   "source": [
    "# IMPLEMENT THE CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68662cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5b5238",
   "metadata": {},
   "source": [
    "# TESTING JIT AND MINIBATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adfiheg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f12083",
   "metadata": {},
   "source": [
    "## JIT + no minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = None\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bed763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c78324",
   "metadata": {},
   "source": [
    "## JIT + Time Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4749a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3b175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c2d5b",
   "metadata": {},
   "source": [
    "## no JIT + time minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "#         reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeca908",
   "metadata": {},
   "source": [
    "## no JIT + no minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = None\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49110d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "#         reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5033",
   "metadata": {},
   "source": [
    "## JIT + space minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc03b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a6796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79357a05",
   "metadata": {},
   "source": [
    "## no JIT + space minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9be80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f5cd90d",
   "metadata": {},
   "source": [
    "# INFINITE HORIZON: ONLINE LEARNING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bf0c",
   "metadata": {},
   "source": [
    "## Validation for the model\n",
    "\n",
    "the Kalman filter usually has the following loop:\n",
    "\n",
    "- For i in infinity:\n",
    "    - Get values of t, R (just add one to t and maintain the same R)\n",
    "    - model.predict(X = t, R = R)\n",
    "    - model.some_update_fn(Y = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc0dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_test, var_test = model.predict_y(X = t_test[0:2], R = R_test[0:2])\n",
    "for i in range(2, 50, 2):\n",
    "    mean_test_i, var_test_i = model.predict_y(X = t_test[i:i+5], R = R_test[i:i+5])\n",
    "    mean_test = np.concatenate([mean_test, mean_test_i])\n",
    "    var_test = np.concatenate([var_test, var_test_i])\n",
    "    model.update_posterior()\n",
    "    model.update_variational_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1296b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_mean_ts, posterior_var_ts = model.predict_y(X=t, R=R_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6f995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d190e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.posterior_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693228f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = model.posterior_mean\n",
    "model.X = np.append(t_train, t_test[0])\n",
    "model.Y = np.append(Y_train, Y_test[0])\n",
    "model.R = np.append(R_train_scaled, R_test_scaled[0])\n",
    "model.update_posterior()\n",
    "b = model.posterior_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae831113",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a == b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c201e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.posterior_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_variational_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.filter(1, model.kernel, 0.5, model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6de79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_mean = model.compute_full_pseudo_lik()[0]\n",
    "latent_var = model.compute_full_pseudo_lik()[1]\n",
    "\n",
    "predictive_pseudo = model.likelihood.predict(latent_mean, latent_var, model.mask_pseudo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS METHOD GETS PREDICTIONS DONE \n",
    "\n",
    "# lik = bayesnewton.likelihoods.Gaussian(variance=VAR_Y)\n",
    "# inf = bayesnewton.inference.Taylor\n",
    "# mod = bayesnewton.basemodels.MarkovGP\n",
    "# Mod = bayesnewton.build_model(mod, inf)\n",
    "# model = Mod(kernel=kern, likelihood=lik, X=t_train, Y=Y_train, R=R_train_scaled)\n",
    "\n",
    "# model.inference()\n",
    "# peudo_y = model.compute_full_pseudo_lik()[0]\n",
    "# noise_cov = model.compute_full_pseudo_lik()[1]\n",
    "# model.update_posterior()\n",
    "# log_lik, (filter_mean, filter_cov) = model.filter(model.dt, model.kernel, peudo_y, noise_cov)\n",
    "\n",
    "# filter_mean[:,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a957fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
