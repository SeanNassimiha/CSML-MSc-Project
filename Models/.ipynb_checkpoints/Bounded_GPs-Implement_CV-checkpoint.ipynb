{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9408a59",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import bayesnewton\n",
    "import jax\n",
    "import objax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import math   \n",
    "from jax import vmap\n",
    "from scipy.stats import beta\n",
    "import wandb\n",
    "\n",
    "import cv2\n",
    "import sys, os\n",
    "sys.path.append('../Utils')\n",
    "import model_utils as mutils\n",
    "import kernels_definitions as kerns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de519bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnassimiha\u001b[0m (\u001b[33mucl_nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437e0fc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#DATA VARIABLES\n",
    "SYSTEMS_NUM = 600\n",
    "TIMESTEPS_NUM = 50000\n",
    "TRAIN_FRAC = 24  #IF TRAIN_FRAC > 1 THEN IT BECOMES THE LENGTH OF THE TEST SET\n",
    "GRID_PIXELS = 10\n",
    "\n",
    "#OPTIMISATION VARIABLES\n",
    "LR_ADAM = 0.05\n",
    "LR_NEWTON = 0.5\n",
    "ITERS = 500\n",
    "\n",
    "# GP Variables\n",
    "BETA_SCALE = 130\n",
    "\n",
    "#PERIODIC KERNEL\n",
    "VAR_PERIOD = 0.271241\n",
    "VAR_MATERN = 3.688516\n",
    "LEN_MATERN = 0.873962\n",
    "LEN_MATERN_PERIOD =   163.474460\n",
    "LEN_PERIOD = 1.329650\n",
    "LEN_SPACE = 0.2\n",
    "\n",
    "Z_EVERY_N = 3\n",
    "\n",
    "#Want to use a sparse approximation\n",
    "SPARSE = True\n",
    "#Should we optimise the inducing points\n",
    "OPT_Z = False  # will be set to False if SPARSE=SPARSE\n",
    "\n",
    "#use a mean field approximation?\n",
    "MEAN_FIELD = False\n",
    "MINI_BATCH_SIZE = None #none if you don't want them\n",
    "TEST_STATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874b4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime')\n",
    "uk_pv = pd.read_csv('../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "#data_multiple.plot(legend=False)\n",
    "lats = dict(uk_pv.set_index('ss_id')['latitude_noisy'])\n",
    "longs = dict(uk_pv.set_index('ss_id')['longitude_noisy'])\n",
    "data_multiple = data.iloc[:, :SYSTEMS_NUM][:TIMESTEPS_NUM].reset_index()\n",
    "stacked = mutils.stack_dataframe(data_multiple, lats, longs)\n",
    "#EXTRACT AREAS AROUND LONDON\n",
    "stacked = stacked[(stacked.latitude < 52.5) & (stacked.latitude > 50.5) & (stacked.longitude > -1) & (stacked.longitude < 1)]\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.columns)].set_index('ss_id_string')['kwp'].values * 1000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc04a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(stacked[['epoch', 'longitude', 'latitude']])\n",
    "Y = np.array(stacked[['PV']])\n",
    "\n",
    "# convert to easting and northings\n",
    "british_national_grid_coords = convert_bng(X[:, 1], X[:, 2])\n",
    "X = np.vstack([X[:, 0],\n",
    "              np.array(british_national_grid_coords[0]),\n",
    "              np.array(british_national_grid_coords[1])]).T\n",
    "\n",
    "#Create a space-time grid from X and Y\n",
    "t, R, Y = bayesnewton.utils.create_spatiotemporal_grid(X, Y)\n",
    "#SCALING THE t HERE\n",
    "t = t / (TIMESTEPS_NUM / 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457fd888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/geopandas/array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Grid of initial inducing points')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train test split for 3 dimensional data\n",
    "t_train, t_test, R_train, R_test, Y_train, Y_test = mutils.train_split_3d(t, R, Y, train_frac = TRAIN_FRAC, split_type = 'Cutoff')\n",
    "\n",
    "#get the mask of the test points\n",
    "test_mask = np.in1d(t.squeeze(), t_test.squeeze())\n",
    "\n",
    "#Scale the data\n",
    "scaled_values = mutils.scale_2d_train_test_data(R, Y, R_train, R_test, Y_train, Y_test )\n",
    "R_scaler, R_scaled, R_train_scaled, R_test_scaled, Y_scaler, Y_scaled, Y_train_scaled, Y_test_scaled, = scaled_values\n",
    "\n",
    "#here get a list of scaled coordinates (frozen because at some point in time)\n",
    "R_scaled_frozen = R_scaled[0]\n",
    "\n",
    "# #Create a grid to perform prediction/interpolation on\n",
    "r1, r2, Rplot = mutils.create_grid_from_coords(R = R_scaled_frozen, t = t, R_scaler = R_scaler, N_pixels = GRID_PIXELS, date_solar = False)\n",
    "\n",
    "# z = R_scaled[2, ...]\n",
    "z =  R_scaled[2, ::Z_EVERY_N]\n",
    "\n",
    "# #CHANGE THE INDUCING POINTS FOR THE SOLAR ALTITUDE TO BE EQUALLY SPACED ALONG THE TOTAL INTERVAL\n",
    "# z[:,2] = np.linspace(solar_altitude.min(),solar_altitude.max(),  len(z))\n",
    "    \n",
    "    \n",
    "plt.scatter(*zip(*z[:, :2]), marker='o', s=30, color='red')\n",
    "plt.title('Grid of initial inducing points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759066f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXED WINDOW OF 5000 train and 24 test, the 5000 train slide forward\n",
    "length_window = 96 * 10\n",
    "max_t = 14000 #len(data_multiple) - length_window - 24\n",
    "#HERE BUILDING ARRAY OF STARTING ts\n",
    "data_multiple.index = pd.to_datetime(data_multiple.index)\n",
    "        \n",
    "# range_idx = np.array([ 1806,  2449,  3043,  3637,  4280,  4874,  5468,  6062,  6705,\n",
    "#         7299,  7893,  8487,  9130,  9723, 10366, 10960, 11554, 12148,\n",
    "#        12791, 13385, 13979, 14573, 15216, 15810, 16393, 17031, 17625,\n",
    "#        18219, 18813, 19456, 20050, 20644, 21238, 21881, 22475, 23069,\n",
    "#        23663, 24306, 24900, 25494, 26088, 26731, 27325, 27919, 28538,\n",
    "#        29132, 29726, 30320, 30963, 31557, 32151, 32745, 33388, 33982,\n",
    "#        34576, 35170, 35813, 36407, 37001, 37595, 38238, 38832, 39426,\n",
    "#        40020, 40663, 41257, 41851, 42494, 43088, 43682, 44295, 44888,\n",
    "#        45531, 46125, 46706, 47288, 47926, 48520, 49101, 49744])\n",
    "\n",
    "        \n",
    "range_idx = np.array([17031, 17625, 18219, 18813, 19456, 20050, 20644, \n",
    "                      21238, 21881, 22475, 23069,\n",
    "       23663, 24306, 24900, 25494, 26088, 26731, 27325, 27919, 28538,\n",
    "       29132, 29726, 30320, 30963, 31557, 32151, 32745, 33388, 33982,\n",
    "       34576, 35170, 35813, 36407, 37001, 37595, 38238, 38832, 39426,\n",
    "       40020, 40663, 41257, 41851, 42494, 43088, 43682, 44295, 44888,\n",
    "       45531, 46125, 46706, 47288, 47926, 48520, 49101, 49744])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d8faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_periodic_kernel(variance_period, variance_matern, lengthscale_time_period, lengthscale_time_matern, lengthscale_time_matern_QP, \n",
    "                        lengthscale_space, z, sparse, opt_z, timestep_n, conditional='Full', matern_order='32', order=6):\n",
    "\n",
    "    if matern_order == '12':\n",
    "        kern_time_period = bayesnewton.kernels.QuasiPeriodicMatern12(variance= variance_period,\n",
    "                                                                  lengthscale_periodic=lengthscale_time_period,\n",
    "                                                                  period=96 / (timestep_n),\n",
    "                                                                  lengthscale_matern= lengthscale_time_matern_QP,\n",
    "                                                                  order=order)\n",
    "\n",
    "        kern_time_matern = bayesnewton.kernels.Matern32(variance=variance_matern, lengthscale=lengthscale_time_matern)\n",
    "\n",
    "        kern_time_day = bayesnewton.kernels.Sum([kern_time_period, kern_time_matern])\n",
    "\n",
    "    elif matern_order == '32':\n",
    "\n",
    "        kern_time_period = bayesnewton.kernels.QuasiPeriodicMatern32(variance= variance_period,\n",
    "                                                                  lengthscale_periodic= lengthscale_time_period,\n",
    "                                                                  period=96/ (timestep_n),\n",
    "                                                                  lengthscale_matern= lengthscale_time_matern_QP,\n",
    "                                                                  order=order)\n",
    "\n",
    "        kern_time_matern = bayesnewton.kernels.Matern32(variance= variance_matern, lengthscale=lengthscale_time_matern)\n",
    "\n",
    "\n",
    "        kern_time_day = bayesnewton.kernels.Sum([kern_time_period, kern_time_matern])\n",
    "\n",
    "\n",
    "    kern_space0 = bayesnewton.kernels.Matern32(variance=variance_period , lengthscale=lengthscale_space)\n",
    "    kern_space1 = bayesnewton.kernels.Matern32(variance=variance_period , lengthscale=lengthscale_space)\n",
    "    kern_space = bayesnewton.kernels.Separable([kern_space0, kern_space1])\n",
    "\n",
    "    kern = bayesnewton.kernels.SpatioTemporalKernel(temporal_kernel=kern_time_day,\n",
    "                                                    spatial_kernel=kern_space,\n",
    "                                                    z=z,\n",
    "                                                    sparse=sparse,\n",
    "                                                    opt_z=opt_z,\n",
    "                                                    conditional=conditional)\n",
    "    return kern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2391a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'variance_period' : VAR_PERIOD, \n",
    "    'variance_matern' : VAR_MATERN, \n",
    "    'lengthscale_time_period' : LEN_PERIOD, \n",
    "    'lengthscale_time_matern' : LEN_MATERN,\n",
    "    'lengthscale_time_matern_QP' : LEN_MATERN_PERIOD,\n",
    "    'lengthscale_space':LEN_SPACE, \n",
    "    'z_every_n': Z_EVERY_N,\n",
    "    'sparse':True,\n",
    "    'opt_z':True,\n",
    "    'timestep_n' : TIMESTEPS_NUM,\n",
    "    'conditional':'Full',\n",
    "    'matern_order' : '32',\n",
    "    'order': 2,\n",
    "    'SYSTEMS_NUM' : SYSTEMS_NUM,\n",
    "\n",
    "    #OPTIMISATION VARIABLES\n",
    "    'LR_ADAM' : LR_ADAM,\n",
    "    'LR_NEWTON' : LR_NEWTON,\n",
    "    'ITERS' : ITERS,\n",
    "\n",
    "    # GP Variables\n",
    "    'BETA_SCALE' : BETA_SCALE,\n",
    "    'MEAN_FIELD' : MEAN_FIELD ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698eca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_init_new = {'(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_lengthscale_periodic': 3.19,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_variance' : 2.217,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_period'  :-4.886,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_lengthscale_matern' : 134.724,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel1(Matern32).transformed_lengthscale'   : -9.314,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel1(Matern32).transformed_variance'  :  21.046,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_lengthscale_lat'  : -0.2601,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_lengthscale_lon'  : -0.2601,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_variance' : -2.448 ,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_lengthscale_lat' : -0.7179,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_lengthscale_lon' : -0.7179,\n",
    "'(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_variance'  : -2.448 ,\n",
    "'(MarkovVariationalGP).likelihood(Beta).transformed_scale':  88.063  }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02a4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.log(1 + np.exp(pd.DataFrame.from_dict(dict_init_new,orient='index').apply(pd.to_numeric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b59d3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VAR_PERIOD = 2.320400\n",
    "VAR_MATERN = 21.046000\n",
    "LEN_PERIOD = 3.230347\n",
    "LEN_MATERN = 0.000090\n",
    "LEN_MATERN_PERIOD = 134.724000\n",
    "LEN_SPACE = 0.571530\n",
    "BETA_SCALE = 88.063000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea166dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for Gaussian Process\n",
      "NEW ITERATION WITH t: 17031\n",
      "TRAIN SIZE IS: t_train_CV: (960, 1), R_train_scaled_CV: (960, 27, 2), Y_train_CV :(960, 27, 1)\n",
      "TEST SIZE IS: t_test_CV: (24, 1), R_test_scaled_CV: (24, 27, 2), Y_test_CV :(24, 27, 1)\n",
      "BEGIN TRAINING\n",
      "iter  1, energy: 445853.0987\n",
      "iter  2, energy: 392450.4668\n",
      "iter  3, energy: 354991.0110\n",
      "iter  4, energy: 343433.0510\n",
      "iter  5, energy: 334244.3570\n",
      "iter  6, energy: 326741.6035\n",
      "iter  7, energy: 319213.9852\n",
      "iter  8, energy: 310724.7918\n",
      "iter  9, energy: 303169.4058\n",
      "iter 10, energy: 295910.8096\n",
      "iter 11, energy: 288372.9168\n",
      "iter 12, energy: 281057.5598\n",
      "iter 13, energy: 273748.0817\n",
      "iter 14, energy: 266657.1203\n",
      "iter 15, energy: 259718.3061\n",
      "iter 16, energy: 252875.8507\n",
      "iter 17, energy: 246128.1612\n",
      "iter 18, energy: 239480.2106\n",
      "iter 19, energy: 232938.8397\n",
      "iter 20, energy: 226509.4550\n",
      "iter 21, energy: 220194.9808\n",
      "iter 22, energy: 213999.5849\n",
      "iter 23, energy: 207927.9958\n",
      "iter 24, energy: 201985.7568\n",
      "iter 25, energy: 196175.8359\n",
      "iter 26, energy: 190498.4330\n",
      "iter 27, energy: 184953.5668\n",
      "iter 28, energy: 179541.5185\n",
      "iter 29, energy: 174262.4396\n",
      "iter 30, energy: 169116.1686\n",
      "iter 31, energy: 164101.7677\n",
      "iter 32, energy: 159217.4167\n",
      "iter 33, energy: 154462.4962\n",
      "iter 34, energy: 149842.6805\n",
      "iter 35, energy: 145366.5604\n",
      "iter 36, energy: 141036.4332\n",
      "iter 37, energy: 136842.6973\n",
      "iter 38, energy: 132782.2066\n",
      "iter 39, energy: 128853.2992\n",
      "iter 40, energy: 125054.3275\n",
      "iter 41, energy: 121383.4919\n",
      "iter 42, energy: 117838.8235\n",
      "iter 43, energy: 114418.1930\n",
      "iter 44, energy: 111119.3223\n",
      "iter 45, energy: 107939.7941\n",
      "iter 46, energy: 104877.0710\n",
      "iter 47, energy: 101928.4999\n",
      "iter 48, energy: 99091.3334\n",
      "iter 49, energy: 96362.7356\n",
      "iter 50, energy: 93739.8000\n",
      "optimisation time: 50.89 secs\n",
      "Performing the predictions\n",
      "mae is 0.12533764740823586 \n",
      "\n",
      "NEW ITERATION WITH t: 17625\n",
      "TRAIN SIZE IS: t_train_CV: (960, 1), R_train_scaled_CV: (960, 27, 2), Y_train_CV :(960, 27, 1)\n",
      "TEST SIZE IS: t_test_CV: (24, 1), R_test_scaled_CV: (24, 27, 2), Y_test_CV :(24, 27, 1)\n",
      "Warm starting the training\n",
      "BEGIN TRAINING\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(f'Getting results for Gaussian Process')\n",
    "\n",
    "errors = np.zeros((24, 1))\n",
    "NNLs = np.zeros((24, 1))\n",
    "\n",
    "forecast_size = 24\n",
    "predictions = np.zeros((24, R_scaled.shape[1], 1))\n",
    "lower_predictions = np.zeros((24, R_scaled.shape[1], 1))\n",
    "upper_predictions = np.zeros((24, R_scaled.shape[1], 1))\n",
    "likelihoods_array = np.zeros((24, R_scaled.shape[1], 1))\n",
    "\n",
    "for iter_idx, t_idx in enumerate(range_idx): \n",
    "    wandb.init(project = 'Cross Validation', group=\"FIRST_RUN_MAC_PERIODIC_21z\", config = config)\n",
    "    print('NEW ITERATION WITH t:',t_idx)\n",
    "\n",
    "    t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "    t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "    t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  \n",
    "    print(f'TRAIN SIZE IS: t_train_CV: {t_train_CV.shape}, R_train_scaled_CV: {R_train_scaled_CV.shape}, Y_train_CV :{Y_train_CV.shape}')\n",
    "    print(f'TEST SIZE IS: t_test_CV: {t_test_CV.shape}, R_test_scaled_CV: {R_test_scaled_CV.shape}, Y_test_CV :{Y_test_CV.shape}')\n",
    "\n",
    "    #IF WE ARE IN THE FIRST ITERATION\n",
    "    if t_idx == range_idx[0]:\n",
    "\n",
    "        kern = get_periodic_kernel(variance_period = VAR_PERIOD, \n",
    "                                        variance_matern = VAR_MATERN, \n",
    "                                        lengthscale_time_period = LEN_PERIOD, \n",
    "                                        lengthscale_time_matern = LEN_MATERN,\n",
    "                                        lengthscale_time_matern_QP = LEN_MATERN_PERIOD,\n",
    "                                        lengthscale_space=[LEN_SPACE, LEN_SPACE], \n",
    "                                        z=z,\n",
    "                                        sparse=True,\n",
    "                                        opt_z=True,\n",
    "                                        timestep_n = TIMESTEPS_NUM,\n",
    "                                        conditional='Full',\n",
    "                                        matern_order = '32',\n",
    "                                        order= 2)\n",
    "\n",
    "        lik = bayesnewton.likelihoods.Beta(scale = BETA_SCALE, fix_scale=False, link='probit')\n",
    "\n",
    "    model = bayesnewton.models.MarkovVariationalGP(kernel = kern, likelihood = lik, X=t_train_CV, Y=Y_train_CV, R=R_train_scaled_CV)\n",
    "\n",
    "    #IF WE ARE NOT IN THE FIRST ITERATION, WARM-START THE TRAINING\n",
    "    if t_idx != range_idx[0]:\n",
    "        print('Warm starting the training')\n",
    "        #HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "        for key in model.vars().keys():\n",
    "            if model.vars()[key].shape  == ():\n",
    "                continue\n",
    "            else:\n",
    "                if model.vars()[key].shape[0] == len(t_train_CV):\n",
    "                    shared_var = model.vars()[key] \n",
    "                    iter_step = np.diff(range_idx)[iter_idx] #difference between time-steps\n",
    "                    init_array = jax.numpy.pad(previous_model.vars()[key][iter_step:], ((0,iter_step), (0,0), (0,0)))\n",
    "                    shared_var.assign(init_array) \n",
    "\n",
    "\n",
    "    opt_hypers = objax.optimizer.Adam(model.vars())\n",
    "    energy = objax.GradValues(model.energy, model.vars())\n",
    "\n",
    "    @objax.Function.with_vars(model.vars() + opt_hypers.vars())\n",
    "    def train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "        dE, E = energy()  # compute energy and its gradients w.r.t. hypers\n",
    "        opt_hypers(LR_ADAM, dE)\n",
    "        return E\n",
    "    train_op = objax.Jit(train_op)\n",
    "\n",
    "    print('BEGIN TRAINING')\n",
    "    t0 = time.time()\n",
    "    #DOING HALF THE ITERATIONS WHEN UPDATING THE MODEL\n",
    "    iterations_n = ITERS if t_idx == range_idx[0] else int(ITERS/10) \n",
    "    for i in range(1, iterations_n + 1):\n",
    "        E = train_op(None)[0].item()\n",
    "        print('iter %2d, energy: %1.4f' % (i, E ))\n",
    "\n",
    "        #LOG THE MODEL PARAMETERS \n",
    "\n",
    "        data_var = [model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_lengthscale_periodic'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_variance'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_period'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_lengthscale_matern'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel1(Matern32).transformed_lengthscale'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel1(Matern32).transformed_variance'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_lengthscale'][0].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_lengthscale'][1].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_variance'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_lengthscale'][0].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_lengthscale'][1].item(),\n",
    "        model.vars()['(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_variance'].item(),\n",
    "        model.vars()['(MarkovVariationalGP).likelihood(Beta).transformed_scale'].item()]\n",
    "\n",
    "        wandb.log({'Energy':E, \n",
    "                   '(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_lengthscale_periodic': data_var[0],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_variance' : data_var[1],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_period'  :data_var[2],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel0(QuasiPeriodicMatern32).transformed_lengthscale_matern' : data_var[3],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel1(Matern32).transformed_lengthscale'   : data_var[4] ,\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).temporal_kernel(Sum).kernel1(Matern32).transformed_variance'  :  data_var[5],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_lengthscale_lat'  : data_var[6],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_lengthscale_lon'  : data_var[7],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel0(Matern32).transformed_variance' : data_var[8] ,\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_lengthscale_lat' : data_var[9],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_lengthscale_lon' : data_var[10],\n",
    "                    '(MarkovVariationalGP).kernel(SpatioTemporalKernel).spatial_kernel(Separable).kernel1(Matern32).transformed_variance'  : data_var[11] ,\n",
    "                    '(MarkovVariationalGP).likelihood(Beta).transformed_scale':  data_var[12]  })\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "\n",
    "    print('Performing the predictions')\n",
    "    #GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "\n",
    "    f_mean, f_var = model.predict(X=t_test_CV, R=R_test_scaled_CV)\n",
    "\n",
    "    #################GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "    f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "    f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "    mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "    posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "#     ################## DELETE ANY PREDICTION THAT IS ALL 0.5 at 2 S.F.\n",
    "#     faulty_systems_idx = np.where((round(posterior_mean_ts, 4) == 0.5).all(axis=0))\n",
    "#     posterior_mean_ts = posterior_mean_ts.at[:, faulty_systems_idx].set(np.nan)\n",
    "\n",
    "    ##################GET THE ERRORS\n",
    "    error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "    print(f'mae is {error.mean()} \\n')\n",
    "\n",
    "    errors = np.concatenate((errors, error), axis=1)  \n",
    "\n",
    "    #################### GET THE NNL\n",
    "\n",
    "    #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "    N_samples = 1000\n",
    "    #Sample values of f at each point\n",
    "    sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "    alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "    beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "    #STORE THE UNC BOUNDS\n",
    "    beta_samples = np.random.beta(alpha_sampled, beta_sampled, size=(alpha_sampled.shape[0], alpha_sampled.shape[1], alpha_sampled.shape[2]))\n",
    "    lower_bounds_beta_MC = np.quantile(beta_samples, 0.025, axis=0)\n",
    "    upper_bounds_beta_MC = np.quantile(beta_samples, 0.975, axis=0)\n",
    "\n",
    "    #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "    observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)[:,:,:,0]\n",
    "    observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "    likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "    NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "\n",
    "    NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "    #####################\n",
    "\n",
    "    previous_model = model\n",
    "\n",
    "    #SAVE AT EACH ITERATION THE FOLLOWING 4 DATASETS\n",
    "\n",
    "    likelihoods_array = np.concatenate((likelihoods_array, likelihoods.mean(axis=0)[:, :, np.newaxis]), axis=2) \n",
    "    lower_predictions = np.concatenate((lower_predictions, lower_bounds_beta_MC[:, :, np.newaxis]), axis=2) \n",
    "    upper_predictions = np.concatenate((upper_predictions, upper_bounds_beta_MC[:, :, np.newaxis]), axis=2) \n",
    "    predictions = np.concatenate((predictions, posterior_mean_ts[:, :, np.newaxis]), axis=2) \n",
    "    \n",
    "    #SAVE AT EACH ITERATION THE FOLLOWING 4 DATASETS\n",
    "    np.save('output/second_run_periodic_21z/partial_predictions.npy', predictions)\n",
    "    np.save('output/second_run_periodic_21z/partial_lower_predictions.npy', lower_predictions)\n",
    "    np.save('output/second_run_periodic_21z/partial_upper_predictions.npy', upper_predictions)\n",
    "    np.save('output/second_run_periodic_21z/partial_likelihoods_array.npy', likelihoods_array)\n",
    "    np.save('output/second_run_periodic_21z/partial_errors.npy', errors)\n",
    "    np.save('output/second_run_periodic_21z/partial_NNLs.npy', NNLs)\n",
    "\n",
    "    wandb.log({'error':np.array(error).mean(), 'NNL_hsteps':np.array(NNL_hsteps).mean(), 'posterior_mean_ts':posterior_mean_ts, 'lower_bounds_beta_MC':lower_bounds_beta_MC, 'upper_bounds_beta_MC':upper_bounds_beta_MC,  'likelihoods.mean(axis=0)': likelihoods.mean(axis=0)})\n",
    "\n",
    "    wandb.run.summary['error'] = error\n",
    "    wandb.run.summary['NNL_hsteps'] = NNL_hsteps\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    del t_iter, R_iter, Y_iter, t_train_CV, R_train_scaled_CV, Y_train_CV, t_test_CV, R_test_scaled_CV, Y_test_CV\n",
    "    del NNL_hsteps, observed_repeated, beta_sampled, alpha_sampled, sampled_f, error, posterior_mean_ts, posterior_var_ts\n",
    "    del mean_y, var_y, f_mean, f_var, likelihoods, model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8853e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t1 = time.time()\n",
    "# print(f'Getting results for Gaussian Process')\n",
    "\n",
    "# errors = np.zeros((24, 1))\n",
    "# NNLs = np.zeros((24, 1))\n",
    "\n",
    "# forecast_size = 24\n",
    "# predictions = np.zeros((24, R_scaled.shape[1], 1))\n",
    "# lower_predictions = np.zeros((24, R_scaled.shape[1], 1))\n",
    "# upper_predictions = np.zeros((24, R_scaled.shape[1], 1))\n",
    "\n",
    "# likelihoods_array = np.zeros((24, R_scaled.shape[1], 1))\n",
    "\n",
    "# for iter_idx, t_idx in enumerate(range_idx[:3]): \n",
    "#     print('NEW ITERATION WITH t:',t_idx)\n",
    "        \n",
    "#     t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "#     t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "#     t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  \n",
    "#     print(f'TRAIN SIZE IS: t_train_CV: {t_train_CV.shape}, R_train_scaled_CV: {R_train_scaled_CV.shape}, Y_train_CV :{Y_train_CV.shape}')\n",
    "#     print(f'TEST SIZE IS: t_test_CV: {t_test_CV.shape}, R_test_scaled_CV: {R_test_scaled_CV.shape}, Y_test_CV :{Y_test_CV.shape}')\n",
    "\n",
    "#     #IF WE ARE IN THE FIRST ITERATION\n",
    "#     if t_idx == range_idx[0]:\n",
    "        \n",
    "#         kern = kerns.get_periodic_kernel(variance_period = VAR_PERIOD, \n",
    "#                                          variance_matern = VAR_MATERN, \n",
    "#                                          lengthscale_time_period = LEN_PERIOD, \n",
    "#                                          lengthscale_time_matern = LEN_MATERN,\n",
    "#                                          lengthscale_space=[LEN_SPACE, LEN_SPACE], \n",
    "#                                          z=z,\n",
    "#                                          sparse=SPARSE,\n",
    "#                                          opt_z=OPT_Z,\n",
    "#                                          conditional='Full',\n",
    "#                                          matern_order = '32',\n",
    "#                                          timestep_n = TIMESTEPS_NUM,\n",
    "#                                          order= 2)\n",
    "\n",
    "#         lik = bayesnewton.likelihoods.Beta(scale = BETA_SCALE, fix_scale=False, link='probit')\n",
    "    \n",
    "#     model = bayesnewton.models.MarkovVariationalGP(kernel = kern, likelihood = lik, X=t_train_CV, Y=Y_train_CV, R=R_train_scaled_CV)\n",
    "    \n",
    "#     #IF WE ARE NOT IN THE FIRST ITERATION, WARM-START THE TRAINING\n",
    "#     if t_idx != range_idx[0]:\n",
    "#         print('Warm starting the training')\n",
    "#         #HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "#         for key in model.vars().keys():\n",
    "#             if model.vars()[key].shape  == ():\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 if model.vars()[key].shape[0] == len(t_train_CV):\n",
    "#                     shared_var = model.vars()[key] \n",
    "#                     iter_step = np.diff(range_idx)[iter_idx] #difference between time-steps\n",
    "#                     init_array = jax.numpy.pad(previous_model.vars()[key][iter_step:], ((0,iter_step), (0,0), (0,0)))\n",
    "#                     shared_var.assign(init_array) \n",
    "\n",
    "    \n",
    "#     opt_hypers = objax.optimizer.Adam(model.vars())\n",
    "#     energy = objax.GradValues(model.energy, model.vars())\n",
    "    \n",
    "#     @objax.Function.with_vars(model.vars() + opt_hypers.vars())\n",
    "#     def train_op(batch_ind = None):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "#         dE, E = energy()  # compute energy and its gradients w.r.t. hypers\n",
    "#         opt_hypers(LR_ADAM, dE)\n",
    "#     train_op = objax.Jit(train_op)\n",
    "\n",
    "#     @objax.Function.with_vars(model.vars())\n",
    "#     def reduced_train_op(batch_ind = None):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "#     reduced_train_op = objax.Jit(reduced_train_op)\n",
    "\n",
    "#     print('BEGIN TRAINING')\n",
    "#     t0 = time.time()\n",
    "#     loss = []\n",
    "#     #DOING HALF THE ITERATIONS WHEN UPDATING THE MODEL\n",
    "#     iterations_n = ITERS if t_idx == range_idx[0] else int(ITERS/2) \n",
    "#     for i in range(1, iterations_n + 1):\n",
    "#         if t_idx == range_idx[0]:\n",
    "#             train_op(None)\n",
    "#         else:\n",
    "#             reduced_train_op(None)\n",
    "#         loss.append(model.compute_kl().item())\n",
    "#         print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "#     t1 = time.time()\n",
    "#     print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "    \n",
    "#     print('Performing the predictions')\n",
    "#     #GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "\n",
    "#     f_mean, f_var = model.predict(X=t_test_CV, R=R_test_scaled_CV)\n",
    "\n",
    "#     #################GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "#     f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "#     f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "#     mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "#     posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "    \n",
    "#     print(posterior_mean_ts.shape)\n",
    "#     predictions = np.concatenate((predictions, posterior_mean_ts[:, :, np.newaxis]), axis=2) \n",
    "    \n",
    "#     ################## DELETE ANY PREDICTION THAT IS ALL 0.5 at 2 S.F.\n",
    "#     faulty_systems_idx = np.where((round(posterior_mean_ts, 2) == 0.5).all(axis=0))\n",
    "#     posterior_mean_ts = posterior_mean_ts.at[:, faulty_systems_idx].set(np.nan)\n",
    "\n",
    "#     ##################GET THE ERRORS\n",
    "#     error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "#     print(f'mae is {error.mean()} \\n')\n",
    "    \n",
    "#     errors = np.concatenate((errors, error), axis=1)  \n",
    "    \n",
    "#     #################### GET THE NNL\n",
    "    \n",
    "#     #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "#     N_samples = 1000\n",
    "#     #Sample values of f at each point\n",
    "#     sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "#     alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "#     beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "#     #STORE THE UNC BOUNDS\n",
    "#     beta_samples = np.random.beta(alpha_sampled, beta_sampled, size=(alpha_sampled.shape[0], alpha_sampled.shape[1], alpha_sampled.shape[2]))\n",
    "#     lower_bounds_beta_MC = np.quantile(beta_samples, 0.025, axis=0)\n",
    "#     upper_bounds_beta_MC = np.quantile(beta_samples, 0.975, axis=0)\n",
    "    \n",
    "#     lower_predictions = np.concatenate((lower_predictions, lower_bounds_beta_MC[:, :, np.newaxis]), axis=2) \n",
    "#     upper_predictions = np.concatenate((upper_predictions, upper_bounds_beta_MC[:, :, np.newaxis]), axis=2) \n",
    "    \n",
    "    \n",
    "#     #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "#     observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)[:,:,:,0]\n",
    "#     observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "#     likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "#     NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "    \n",
    "#     likelihoods_array = np.concatenate((likelihoods_array, likelihoods.mean(axis=0)[:, :, np.newaxis]), axis=2) \n",
    "\n",
    "#     NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "#     #####################\n",
    "    \n",
    "#     previous_model = model\n",
    "    \n",
    "    \n",
    "#     #SAVE AT EACH ITERATION THE FOLLOWING 4 DATASETS\n",
    "#     np.save('output/CV_results_first_run_periodic_7z/partial_predictions.npy', predictions)\n",
    "#     np.save('output/CV_results_first_run_periodic_7z/partial_lower_predictions.npy', lower_predictions)\n",
    "#     np.save('output/CV_results_first_run_periodic_7z/partial_upper_predictions.npy', upper_predictions)\n",
    "#     np.save('output/CV_results_first_run_periodic_7z/partial_likelihoods_array.npy', likelihoods_array)\n",
    "\n",
    "    \n",
    "#     del t_iter, R_iter, Y_iter, t_train_CV, R_train_scaled_CV, Y_train_CV, t_test_CV, R_test_scaled_CV, Y_test_CV\n",
    "#     del NNL_hsteps, observed_repeated, beta_sampled, alpha_sampled, sampled_f, error, posterior_mean_ts, posterior_var_ts\n",
    "#     del mean_y, var_y, f_mean, f_var #, likelihoods, #model\n",
    "    \n",
    "# error_evolution = errors[:, 1:].mean(axis=0)\n",
    "# MAE_hsteps = errors[:, 1:].mean(axis=1)\n",
    "# NNLs_hsteps = np.quantile(NNLs[:, 1:], 0.5, axis=1)\n",
    "# NNLs_hsteps_upper = np.quantile(NNLs[:, 1:], 0.975,  axis=1)\n",
    "# NNLs_hsteps_lower = np.quantile(NNLs[:, 1:], 0.025,  axis=1)\n",
    "\n",
    "\n",
    "# error_evolution = pd.DataFrame(error_evolution).rename(columns ={0:'error_evolution'} )\n",
    "# MAE_hsteps = pd.DataFrame(MAE_hsteps).rename(columns ={0:'MAE_hsteps'} )\n",
    "# NNLs_hsteps = pd.DataFrame(NNLs_hsteps).rename(columns ={0:'NNLs_hsteps'} )\n",
    "\n",
    "# # wandb.log({\"error_evolution\": error_evolution,  \"MAE_hsteps\": MAE_hsteps, \"NNLs_hsteps\": NNLs_hsteps})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f30efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS TO PLOT THE PREDICTIONS FROM THE MODEL ITERATION\n",
    "\n",
    "t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  \n",
    "\n",
    "f_mean, f_var = model.predict(X=t_iter[-500:], R=R_iter[-500:])\n",
    "\n",
    "#################GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_mode(alpha, beta):\n",
    "    '''\n",
    "    Calcualte the mode of the beta distribution given values of alpha and beta\n",
    "    \n",
    "    Use:\n",
    "    (𝑎−1)/(𝑎+𝑏−2)=𝑚𝑜𝑑𝑒\n",
    "    '''\n",
    "    \n",
    "    return (alpha - 1) / (alpha + beta - 2)\n",
    "\n",
    "\n",
    "#Sample values of f at each point\n",
    "sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(500, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "beta_samples = np.random.beta(alpha_sampled, beta_sampled, size=(alpha_sampled.shape[0], alpha_sampled.shape[1], alpha_sampled.shape[2]))\n",
    "lower_bounds_beta_MC = np.quantile(beta_samples, 0.01, axis=0)\n",
    "upper_bounds_beta_MC = np.quantile(beta_samples, 0.99, axis=0)\n",
    "median_MC = np.quantile(beta_samples, 0.5, axis=0)\n",
    "\n",
    "bounds_90_beta_MC = np.quantile(beta_samples, 0.90, axis=0)\n",
    "bounds_10_beta_MC = np.quantile(beta_samples, 0.10, axis=0)\n",
    "\n",
    "mode_beta_MC = beta_mode(alpha_sampled, beta_sampled).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8cb86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = 500\n",
    "\n",
    "for i in range(SYSTEMS_NUM)[:50]:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(f'Prediction for system {i}')\n",
    "\n",
    "    plt.plot(np.arange(len(Y_iter[-history:])), Y_iter[-history:,i], \"xk\", label='Ground truth')\n",
    "    plt.plot(np.arange(len(Y_iter[-history:])), posterior_mean_ts[-history:,i], c=\"C0\", lw=2, zorder=2, label='posterior mean')\n",
    "    plt.plot(np.arange(len(Y_iter[-history:])), median_MC[-history:,i], c=\"red\", lw=2, zorder=2, alpha = 0.5, label='sampled posterior median')\n",
    "#     plt.plot(np.arange(len(Y_iter[-history:])), mode_beta_MC[-history:,i], c=\"orange\", lw=2, zorder=2, alpha = 0.5, linestyle='-.', label='sampled posterior mode')\n",
    "\n",
    "    \n",
    "    plt.vlines(np.arange(len(Y_iter[-history:]))[-24], 0, max(Y_iter[-history:,i].max().item(), upper_bounds_beta_MC[-history:,i].max().item())+0.1, colors='k')\n",
    "\n",
    "    plt.fill_between(\n",
    "        np.arange(len(Y_iter[-history:])),\n",
    "        lower_bounds_beta_MC[-history:,i],\n",
    "        upper_bounds_beta_MC[-history:,i],\n",
    "        color=\"C1\",\n",
    "        alpha=0.2,\n",
    "        label = '1-99 quantiles')\n",
    "    \n",
    "    plt.fill_between(\n",
    "        np.arange(len(Y_iter[-history:])),\n",
    "        bounds_10_beta_MC[-history:,i],\n",
    "        bounds_90_beta_MC[-history:,i],\n",
    "        color=\"green\",\n",
    "        alpha=0.4,\n",
    "        label = '10-90 quantiles')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('time-step')\n",
    "    plt.ylabel('PVE Capacity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################GET THE ERRORS\n",
    "# error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "# print(f'mae is {error.mean()} \\n')\n",
    "\n",
    "# errors = np.concatenate((errors, error), axis=1)  \n",
    "\n",
    "# #################### GET THE NNL\n",
    "\n",
    "# #SAMPLE THE LATENT VARIABLE AND GET THE SAMPLED DISTRIBUTIONS\n",
    "# N_samples = 1000\n",
    "# #Sample values of f at each point\n",
    "# sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(N_samples, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "# alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "# beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "# #GET THE NEGATIVE LOG LIKELIHOOD GIVEN THE SAMPLED DISTRIBUTION AND THE OBSERVED Y VALUES\n",
    "# observed_repeated = np.repeat(Y_test_CV[np.newaxis, :, :], N_samples, axis=0)\n",
    "# observed_repeated = observed_repeated.at[observed_repeated==0].set(10e-6)\n",
    "# likelihoods = beta.pdf(observed_repeated,  alpha_sampled, beta_sampled)\n",
    "# NNL_hsteps = -np.sum(np.log(likelihoods.mean(axis=0)), axis=1)[:, np.newaxis]\n",
    "\n",
    "# NNLs = np.concatenate((NNLs, NNL_hsteps), axis=1)  \n",
    "\n",
    "# #####################\n",
    "\n",
    "# previous_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3459baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(NNLs_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13db7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20384f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MAE_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(NNLs_hsteps).rename(columns ={0:'NNLs_hsteps'} ).to_csv('NNLs_hsteps')\n",
    "pd.DataFrame(error_evolution).rename(columns ={0:'error_evolution'} ).to_csv('error_evolution')\n",
    "pd.DataFrame(MAE_hsteps).rename(columns ={0:'MAE_hsteps'} ).to_csv('MAE_hsteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_iter, R_iter, Y_iter = t[t_idx:t_idx+length_window + 24], R_scaled[t_idx:t_idx+length_window + 24], Y[t_idx:t_idx+length_window + 24]    \n",
    "t_train_CV, R_train_scaled_CV, Y_train_CV = t_iter[:length_window] , R_iter[:length_window] , Y_iter[:length_window]  \n",
    "t_test_CV, R_test_scaled_CV, Y_test_CV = t_iter[length_window:] , R_iter[length_window:] , Y_iter[length_window:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test_CV) + 50\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "#GET THE ERRORS\n",
    "print(f'testing using the next {len(Y[-len_samples:][test_mask_shortened])} datapoints')\n",
    "error = np.nanmean(abs(np.squeeze(Y[-len_samples:][test_mask_shortened]) - np.squeeze(posterior_mean_ts[test_mask_shortened])), axis=1)\n",
    "print(f'mae is {error}')\n",
    "\n",
    "# errors = np.concatenate((errors, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e555",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test) + 500\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "t1 = time.time()\n",
    "print('prediction time: %2.2f secs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de01e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GET THE PREDICTION INTERVALS AND CALCULATE ERRORS\n",
    "\n",
    "posterior_pos_twostd_rescaled = posterior_mean_ts + 1.96 * np.sqrt(posterior_var_ts)\n",
    "posterior_neg_twostd_rescaled = posterior_mean_ts - 1.96 * np.sqrt(posterior_var_ts)\n",
    "\n",
    "rescaled_Y = (Y ) #* capacities)\n",
    "rescaled_posterior = posterior_mean_ts#) #* capacities\n",
    "\n",
    "#adjust this for the correct quantities\n",
    "mae = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:]) - np.squeeze(rescaled_posterior)))\n",
    "print(f'The MAE is {mae.round(3)}')\n",
    "\n",
    "mae_train = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][~test_mask_shortened]) - np.squeeze(rescaled_posterior[~test_mask_shortened])))\n",
    "print(f'The train MAE is {mae_train.round(3)}')\n",
    "\n",
    "mae_test = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][test_mask_shortened]) - np.squeeze(rescaled_posterior[test_mask_shortened])), axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(mae_test)\n",
    "plt.title('Error as function of forecast distance for Gaussian Process on validation test')\n",
    "plt.xlabel('Number of steps ahead (5min ticks)')\n",
    "plt.ylabel('Average MW error')\n",
    "\n",
    "print(f'The average 2 hours test MAE is {mae_test[:24].mean()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
