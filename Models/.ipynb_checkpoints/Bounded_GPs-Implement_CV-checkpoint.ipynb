{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9408a59",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import bayesnewton\n",
    "import jax\n",
    "import objax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import math   \n",
    "from jax import vmap\n",
    "\n",
    "import cv2\n",
    "import sys, os\n",
    "sys.path.append('../Utils')\n",
    "import model_utils as mutils\n",
    "import kernels_definitions as kerns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437e0fc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#DATA VARIABLES\n",
    "SYSTEMS_NUM = 40\n",
    "TIMESTEPS_NUM = 50000\n",
    "TRAIN_FRAC = 2  #IF TRAIN_FRAC > 1 THEN IT BECOMES THE LENGTH OF THE TEST SET\n",
    "GRID_PIXELS = 20\n",
    "\n",
    "#OPTIMISATION VARIABLES\n",
    "LR_ADAM = 0.01\n",
    "LR_NEWTON = 0.5\n",
    "ITERS = 5\n",
    "\n",
    "#GP Variables\n",
    "VAR_Y = 0.8\n",
    "VAR_F = 0.8\n",
    "LEN_TIME = 6  # step size = 1 (hour)\n",
    "LEN_SPACE = 1\n",
    "LEN_ALTITUDE = 0.3\n",
    "\n",
    "#Want to use a sparse approximation\n",
    "SPARSE = True\n",
    "#Should we optimise the inducing points\n",
    "OPT_Z = False  # will be set to False if SPARSE=SPARSE\n",
    "\n",
    "#use a mean field approximation?\n",
    "MEAN_FIELD = True\n",
    "MINI_BATCH_SIZE = None #none if you don't want them\n",
    "TEST_STATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874b4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime').drop(columns=['2657', '2828']) #DROPPING FAULTY SYSTEMS\n",
    "uk_pv = pd.read_csv('../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "#data_multiple.plot(legend=False)\n",
    "lats = dict(uk_pv.set_index('ss_id')['latitude_noisy'])\n",
    "longs = dict(uk_pv.set_index('ss_id')['longitude_noisy'])\n",
    "data_multiple = data.iloc[:, :SYSTEMS_NUM][:TIMESTEPS_NUM].reset_index()\n",
    "stacked = mutils.stack_dataframe(data_multiple, lats, longs)\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.columns)].set_index('ss_id_string')['kwp'].values * 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc04a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(stacked[['epoch', 'longitude', 'latitude']])\n",
    "Y = np.array(stacked[['PV']])\n",
    "\n",
    "from suncalc import get_position, get_times\n",
    "date_solar = stacked.datetime.values\n",
    "lon_solar = stacked.longitude.values\n",
    "lat_solar = stacked.latitude.values\n",
    "\n",
    "solar_positions = get_position(date_solar, lon_solar, lat_solar)\n",
    "solar_altitude = solar_positions['altitude']\n",
    "\n",
    "# convert to easting and northings\n",
    "british_national_grid_coords = convert_bng(X[:, 1], X[:, 2])\n",
    "X = np.vstack([X[:, 0],\n",
    "              np.array(british_national_grid_coords[0]),\n",
    "              np.array(british_national_grid_coords[1])]).T\n",
    "\n",
    "#Create a space-time grid from X and Y\n",
    "t, R, Y = bayesnewton.utils.create_spatiotemporal_grid(X, Y)\n",
    "\n",
    "# solar_altitudes = solar_altitude.reshape(R.shape[1], R.shape[0], 1).swapaxes(0,1)\n",
    "\n",
    "# R = np.append(R, solar_altitudes, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457fd888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SeanNassimiha/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/geopandas/array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Grid of initial inducing points')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train test split for 3 dimensional data\n",
    "t_train, t_test, R_train, R_test, Y_train, Y_test = mutils.train_split_3d(t, R, Y, train_frac = TRAIN_FRAC, split_type = 'Cutoff')\n",
    "Y = Y[:,:,0]\n",
    "\n",
    "#get the mask of the test points\n",
    "test_mask = np.in1d(t.squeeze(), t_test.squeeze())\n",
    "\n",
    "#Scale the data\n",
    "scaled_values = mutils.scale_2d_train_test_data(R, Y, R_train, R_test, Y_train, Y_test )\n",
    "R_scaler, R_scaled, R_train_scaled, R_test_scaled, _, _, _, _ = scaled_values\n",
    "\n",
    "#here get a list of scaled coordinates (frozen because at some point in time)\n",
    "R_scaled_frozen = R_scaled[0]\n",
    "\n",
    "# #Create a grid to perform prediction/interpolation on\n",
    "r1, r2, Rplot = mutils.create_grid_from_coords(R = R_scaled_frozen, t = t, R_scaler = R_scaler, N_pixels = GRID_PIXELS, date_solar = date_solar)\n",
    "\n",
    "z = R_scaled[2, ...]\n",
    "\n",
    "# #CHANGE THE INDUCING POINTS FOR THE SOLAR ALTITUDE TO BE EQUALLY SPACED ALONG THE TOTAL INTERVAL\n",
    "# z[:,2] = np.linspace(solar_altitude.min(),solar_altitude.max(),  len(z))\n",
    "    \n",
    "plt.scatter(*zip(*z[:, :2]), marker='o', s=30, color='red')\n",
    "plt.title('Grid of initial inducing points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b8853e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for Gaussian Process\n",
      "NEW ITERATION WITH t: 10000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (7, 7), (5, 5).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#IF WE ARE IN THE FIRST ITERATION\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t_idx \u001b[38;5;241m==\u001b[39m iter_start:\n\u001b[0;32m---> 25\u001b[0m     kern \u001b[38;5;241m=\u001b[39m \u001b[43mkerns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_periodic_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVAR_F\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mlengthscale_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEN_TIME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mlengthscale_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mLEN_SPACE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEN_SPACE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#[LEN_SPACE, LEN_SPACE, LEN_ALTITUDE]\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPARSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mopt_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPT_Z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mconditional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFIC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     lik \u001b[38;5;241m=\u001b[39m bayesnewton\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mBeta(scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, fix_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, link\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m bayesnewton\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mMarkovVariationalMeanFieldGP(kernel \u001b[38;5;241m=\u001b[39m kern, likelihood \u001b[38;5;241m=\u001b[39m lik, X\u001b[38;5;241m=\u001b[39mt_train_CV, Y\u001b[38;5;241m=\u001b[39mY_train_CV, R\u001b[38;5;241m=\u001b[39mR_train_scaled_CV)\n",
      "File \u001b[0;32m~/Desktop/MSc Project/CSML MSc Project/Models/../Utils/kernels_definitions.py:73\u001b[0m, in \u001b[0;36mget_periodic_kernel\u001b[0;34m(variance, lengthscale_time, lengthscale_space, z, sparse, opt_z, conditional, order)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_periodic_kernel\u001b[39m(variance, lengthscale_time, lengthscale_space, z, sparse, opt_z, conditional\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# kern_time_year = bayesnewton.kernels.QuasiPeriodicMatern32(variance=variance,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m#                                                     lengthscale_periodic = lengthscale_time,\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m#                                                     period = 97 * 365,\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#                                                     lengthscale_matern= lengthscale_time * 100)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     kern_time_day \u001b[38;5;241m=\u001b[39m \u001b[43mbayesnewton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuasiPeriodicMatern32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mlengthscale_periodic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlengthscale_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m97\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mlengthscale_matern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlengthscale_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# kern_time = bayesnewton.kernels.Sum([kern_time_day, kern_time_year])\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     kern_space0 \u001b[38;5;241m=\u001b[39m bayesnewton\u001b[38;5;241m.\u001b[39mkernels\u001b[38;5;241m.\u001b[39mMatern32(variance\u001b[38;5;241m=\u001b[39mvariance, lengthscale\u001b[38;5;241m=\u001b[39mlengthscale_space)\n",
      "File \u001b[0;32m~/Desktop/MSc Project/BayesNewtonPVE/bayesnewton/kernels.py:1186\u001b[0m, in \u001b[0;36mQuasiPeriodicMatern32.__init__\u001b[0;34m(self, variance, lengthscale_periodic, period, lengthscale_matern, order)\u001b[0m\n\u001b[1;32m   1164\u001b[0m factorial_mesh_K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m   1165\u001b[0m     [\n\u001b[1;32m   1166\u001b[0m         [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     ]\n\u001b[1;32m   1174\u001b[0m )\n\u001b[1;32m   1175\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m   1176\u001b[0m     [\n\u001b[1;32m   1177\u001b[0m         [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     ]\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_fmK_2igrid \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfactorial_mesh_K\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43migrid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:6747\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   6745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[1;32m   6746\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m-> 6747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:709\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2):\n\u001b[1;32m    708\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m _promote_args(numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, x1, x2)\n\u001b[0;32m--> 709\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m bool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/mscjax_dev/lib/python3.8/site-packages/jax/_src/lax/lax.py:1422\u001b[0m, in \u001b[0;36m_broadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     non_1s \u001b[38;5;241m=\u001b[39m {d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m ds \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39msymbolic_equal_dim(d, \u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_1s) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1422\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1423\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, shapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1424\u001b[0m     result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s\u001b[38;5;241m.\u001b[39mpop() \u001b[38;5;28;01mif\u001b[39;00m non_1s \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (7, 7), (5, 5)."
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(f'Getting results for Gaussian Process')\n",
    "max_t = 49006\n",
    "iter_start = 10000 \n",
    "iter_step = 5000\n",
    "\n",
    "errors = np.zeros((24, 1))\n",
    "forecast_size = 24\n",
    "\n",
    "for t_idx in range(iter_start, max_t, iter_step): \n",
    "    print('NEW ITERATION WITH t:',t_idx)\n",
    "\n",
    "    t_train_CV = t_train[:t_idx]\n",
    "    t_test_CV = t_train[t_idx: t_idx+forecast_size]\n",
    "    \n",
    "    R_train_scaled_CV = R_train_scaled[:t_idx]\n",
    "    R_test_scaled_CV = R_train_scaled[t_idx: t_idx+forecast_size]\n",
    "    \n",
    "    Y_train_CV = Y_train[:t_idx]\n",
    "    Y_test_CV = Y_train[t_idx: t_idx+forecast_size]\n",
    "\n",
    "    #IF WE ARE IN THE FIRST ITERATION\n",
    "    if t_idx == iter_start:\n",
    "        \n",
    "        kern = kerns.get_periodic_kernel(variance=VAR_F,\n",
    "                                               lengthscale_time=LEN_TIME,\n",
    "                                               lengthscale_space=[LEN_SPACE, LEN_SPACE], #[LEN_SPACE, LEN_SPACE, LEN_ALTITUDE]\n",
    "                                               z=z,\n",
    "                                               sparse=SPARSE,\n",
    "                                               opt_z=OPT_Z,\n",
    "                                               conditional='FIC',\n",
    "                                                order= 4)\n",
    "\n",
    "        lik = bayesnewton.likelihoods.Beta(scale = 30, fix_scale=False, link='probit')\n",
    "    \n",
    "    model = bayesnewton.models.MarkovVariationalMeanFieldGP(kernel = kern, likelihood = lik, X=t_train_CV, Y=Y_train_CV, R=R_train_scaled_CV)\n",
    "    \n",
    "    #IF WE ARE NOT IN THE FIRST ITERATION, WARM-START THE TRAINING\n",
    "    if t_idx != iter_start:\n",
    "        print('Warm starting the training')\n",
    "        #HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "        for key in model.vars().keys():\n",
    "            if model.vars()[key].shape  == ():\n",
    "                continue\n",
    "            else:\n",
    "                if model.vars()[key].shape[0] == len(t_train_CV):\n",
    "                    shared_var = model.vars()[key] \n",
    "                    init_array = jax.numpy.pad(previous_model.vars()[key], ((0,iter_step), (0,0), (0,0)))\n",
    "                    shared_var.assign(init_array) \n",
    "\n",
    "    \n",
    "    opt_hypers = objax.optimizer.Adam(model.vars())\n",
    "    energy = objax.GradValues(model.energy, model.vars())\n",
    "\n",
    "    @objax.Function.with_vars(model.vars())\n",
    "    def train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "        dE, E = energy()  # compute energy and its gradients w.r.t. hypers\n",
    "        opt_hypers(LR_ADAM, dE)\n",
    "    train_op = objax.Jit(train_op)\n",
    "    \n",
    "    @objax.Function.with_vars(model.vars())\n",
    "    def reduced_train_op(batch_ind = None):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "    reduced_train_op = objax.Jit(reduced_train_op)\n",
    "\n",
    "    print('BEGIN TRAINING')\n",
    "    t0 = time.time()\n",
    "    loss = []\n",
    "    #DOING HALF THE ITERATIONS WHEN UPDATING THE MODEL\n",
    "    iterations_n = ITERS if t_idx == iter_start else int(ITERS/2) \n",
    "    for i in range(1, iterations_n + 1):\n",
    "        if t_idx == iter_start:\n",
    "            train_op(None)\n",
    "        else:\n",
    "            reduced_train_op(None)\n",
    "        loss.append(model.compute_kl().item())\n",
    "        print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "    t1 = time.time()\n",
    "    print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "    \n",
    "    print('Performing the predictions')\n",
    "    #GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "\n",
    "    f_mean, f_var = model.predict(X=t_test_CV, R=R_test_scaled_CV)\n",
    "\n",
    "    #GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "    f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "    f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "    mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "    posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "    \n",
    "    #GET THE ERRORS\n",
    "    error = np.nanmean(abs(np.squeeze(Y_test_CV) - np.squeeze(posterior_mean_ts)), axis=1)[:, np.newaxis]\n",
    "    print(f'mae is {error.mean()} \\n')\n",
    "    \n",
    "    errors = np.concatenate((errors, error), axis=1)\n",
    "    \n",
    "    previous_model = model\n",
    "\n",
    "error_evolution = errors[:, 1:].mean(axis=0)\n",
    "MAE_hsteps = errors[:, 1:].mean(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13db7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20384f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MAE_hsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "# len_samples = len(t_test_CV) + 24\n",
    "# test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "# f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "# #GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "# f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "# f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "# mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "# posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "# #GET THE ERRORS\n",
    "# print(f'testing using the next {len(Y[-len_samples:][test_mask_shortened])} datapoints')\n",
    "# error = np.nanmean(abs(np.squeeze(Y[-len_samples:][test_mask_shortened]) - np.squeeze(posterior_mean_ts[test_mask_shortened])), axis=1)\n",
    "# print(f'mae is {error}')\n",
    "\n",
    "# errors = np.concatenate((errors, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e555",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#GET THE SYSTEM SPECIFIC PREDICTIONS (NOT THE TOTAL INTERPOLATION)\n",
    "len_samples = len(t_test) + 500\n",
    "test_mask_shortened = test_mask[-len_samples:]\n",
    "\n",
    "f_mean, f_var = model.predict(X=t[-len_samples:], R=R_scaled[-len_samples:])\n",
    "\n",
    "#GET THE Y PREDICTIONS FROM THE F VALUES\n",
    "f_mean = f_mean.reshape(f_mean.shape[0], -1, 1)\n",
    "f_var = f_var.reshape(f_var.shape[0], -1, 1)\n",
    "\n",
    "mean_y, var_y = vmap(model.likelihood.predict, (0, 0, None))(f_mean, f_var, None)\n",
    "posterior_mean_ts, posterior_var_ts = np.squeeze(mean_y), np.squeeze(var_y)\n",
    "\n",
    "t1 = time.time()\n",
    "print('prediction time: %2.2f secs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de01e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GET THE PREDICTION INTERVALS AND CALCULATE ERRORS\n",
    "\n",
    "posterior_pos_twostd_rescaled = posterior_mean_ts + 1.96 * np.sqrt(posterior_var_ts)\n",
    "posterior_neg_twostd_rescaled = posterior_mean_ts - 1.96 * np.sqrt(posterior_var_ts)\n",
    "\n",
    "rescaled_Y = (Y ) #* capacities)\n",
    "rescaled_posterior = posterior_mean_ts#) #* capacities\n",
    "\n",
    "#adjust this for the correct quantities\n",
    "mae = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:]) - np.squeeze(rescaled_posterior)))\n",
    "print(f'The MAE is {mae.round(3)}')\n",
    "\n",
    "mae_train = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][~test_mask_shortened]) - np.squeeze(rescaled_posterior[~test_mask_shortened])))\n",
    "print(f'The train MAE is {mae_train.round(3)}')\n",
    "\n",
    "mae_test = np.nanmean(abs(np.squeeze(rescaled_Y[-len_samples:][test_mask_shortened]) - np.squeeze(rescaled_posterior[test_mask_shortened])), axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(mae_test)\n",
    "plt.title('Error as function of forecast distance for Gaussian Process on validation test')\n",
    "plt.xlabel('Number of steps ahead (5min ticks)')\n",
    "plt.ylabel('Average MW error')\n",
    "\n",
    "print(f'The average 2 hours test MAE is {mae_test[:24].mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE THE UNCERTAINTY BOUNDS\n",
    "\n",
    "#Sample values of f at each point\n",
    "sampled_f = np.random.normal(f_mean[:,:,0], f_var[:,:,0], size=(10, f_var.shape[0], f_var.shape[1]))\n",
    "\n",
    "alpha_sampled = model.likelihood.link_fn(sampled_f) * model.likelihood.scale\n",
    "beta_sampled = model.likelihood.scale - alpha_sampled\n",
    "\n",
    "beta_samples = np.random.beta(alpha_sampled, beta_sampled, size=(alpha_sampled.shape[0], alpha_sampled.shape[1], alpha_sampled.shape[2]))\n",
    "lower_bounds_beta_MC = np.quantile(beta_samples, 0.025, axis=0)\n",
    "upper_bounds_beta_MC = np.quantile(beta_samples, 0.975, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27642700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days_index = max(97, int(((len_samples / 3) // 97) * 97)) #number of time intervals to match 5 beginnings of days\n",
    "\n",
    "for i in range(SYSTEMS_NUM):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(f'Prediction for system {i}')\n",
    "    plt.plot(np.arange(len(Y))[-len_samples:], Y[:,i][-len_samples:], \"xk\")\n",
    "    plt.plot(np.arange(len(Y))[-len_samples:], posterior_mean_ts[:,i][-len_samples:], c=\"C0\", lw=2, zorder=2)\n",
    "#     plt.plot(np.arange(len(Y))[-len_samples:], R[-len_samples:, i, 2], 'green', alpha = 0.1)\n",
    "    plt.vlines(t_train[-1], 0, 1, colors='k')\n",
    "\n",
    "    plt.fill_between(\n",
    "        np.arange(len(Y))[-len_samples:],\n",
    "        lower_bounds_beta_MC[:,i],\n",
    "        upper_bounds_beta_MC[:,i],\n",
    "        color=\"C1\",\n",
    "        alpha=0.2)\n",
    "    \n",
    "    plt.xticks(ticks = np.arange(len(Y))[-len_samples:-1:days_index], labels = data_multiple.datetime[-len_samples:-1:days_index].values, size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0617cf",
   "metadata": {},
   "source": [
    "# WARM START FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST INITIALISE A SECOND MODEL\n",
    "#THIS INITIALISATION USES THE SAME KERNEL AND LIKELIHOOD!\n",
    "model2 = bayesnewton.models.MarkovVariationalGP(kernel = kern, likelihood = lik, X=t, Y=Y, R=R_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE I AM SUBSTITUTING THE PREDICTIONS ETC FROM THE MODEL IN THE TRAINING LOCATIONS\n",
    "for key in model2.vars().keys():\n",
    "    if model2.vars()[key].shape  == ():\n",
    "        continue\n",
    "    else:\n",
    "        if model2.vars()[key].shape[0] == len(t):\n",
    "            shared_var = model2.vars()[key] \n",
    "            init_array = jax.numpy.pad(model.vars()[key], ((0,len(t) - len(t_train)), (0,0), (0,0)))\n",
    "            shared_var.assign(init_array) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02201d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS WHAT IT LOOKS LIKE IN THE MODEL POSTERIOR\n",
    "plt.plot(model2.vars()['(MarkovVariationalGP).posterior_mean'][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "@objax.Function.with_vars(model2.vars())\n",
    "def reduced_train_op(batch_ind = None):\n",
    "    model2.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "reduced_train_op = objax.Jit(reduced_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, 1 + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        if number_of_minibatches > 1:\n",
    "            print(f'Doing minibatch {mini_batch}')\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model2.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8eb0b0",
   "metadata": {},
   "source": [
    "# Compare it to the case without warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST INITIALISE A SECOND MODEL\n",
    "#THIS INITIALISATION USES THE SAME KERNEL AND LIKELIHOOD!\n",
    "kern3 = kerns.get_periodic_kernel(variance=VAR_F,\n",
    "                                           lengthscale_time=LEN_TIME,\n",
    "                                           lengthscale_space=[LEN_SPACE, LEN_SPACE],\n",
    "                                           z=z,\n",
    "                                           sparse=SPARSE,\n",
    "                                           opt_z=OPT_Z,\n",
    "                                           conditional='FIC')\n",
    "\n",
    "lik3 = bayesnewton.likelihoods.Beta(scale = 30, fix_scale=False, link='probit')\n",
    "model3 = bayesnewton.models.MarkovVariationalGP(kernel = kern3, likelihood = lik3, X=t, Y=Y, R=R_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_hypers3 = objax.optimizer.Adam(model3.vars())\n",
    "energy3 = objax.GradValues(model3.energy, model3.vars())\n",
    "\n",
    "@objax.Function.with_vars(model3.vars())\n",
    "def train_op_model3(batch_ind = None):\n",
    "    model3.inference(lr=LR_NEWTON, batch_ind = batch_ind)  #perform inference and update variational params\n",
    "    dE, E = energy3()  # compute energy and its gradients w.r.t. hypers\n",
    "    opt_hypers3(LR_ADAM, dE)\n",
    "train_op_model3 = objax.Jit(train_op_model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bad32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        if number_of_minibatches > 1:\n",
    "            print(f'Doing minibatch {mini_batch}')\n",
    "        train_op_model3(mini_batches_indices[mini_batch])\n",
    "        loss.append(model3.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e923dc",
   "metadata": {},
   "source": [
    "# IMPLEMENT THE CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68662cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5b5238",
   "metadata": {},
   "source": [
    "# TESTING JIT AND MINIBATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adfiheg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f12083",
   "metadata": {},
   "source": [
    "## JIT + no minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = None\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bed763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c78324",
   "metadata": {},
   "source": [
    "## JIT + Time Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4749a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3b175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "#         model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "        reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c2d5b",
   "metadata": {},
   "source": [
    "## no JIT + time minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "#         reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeca908",
   "metadata": {},
   "source": [
    "## no JIT + no minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = None\n",
    "\n",
    "if MINI_BATCH_SIZE == None:\n",
    "    number_of_minibatches = 1\n",
    "    mini_batches_indices = [None] * number_of_minibatches\n",
    "else:\n",
    "    number_of_minibatches = int(len(t_train) / MINI_BATCH_SIZE)\n",
    "    idx_set = np.arange(len(t_train))\n",
    "    np.random.shuffle(idx_set)\n",
    "    mini_batches_indices = np.array_split(idx_set, number_of_minibatches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49110d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "t0 = time.time()\n",
    "loss = []\n",
    "for i in range(1, ITERS + 1):\n",
    "    for mini_batch in range(number_of_minibatches):\n",
    "        model.inference(lr=LR_NEWTON, batch_ind = mini_batches_indices[mini_batch])  #perform inference and update variational params\n",
    "#         reduced_train_op(mini_batches_indices[mini_batch])\n",
    "        loss.append(model.compute_kl().item())\n",
    "    print('iter %2d, energy: %1.4f' % (i, loss[i-1]))\n",
    "t1 = time.time()\n",
    "print('optimisation time: %2.2f secs' % (t1-t0))\n",
    "avg_time_taken = (t1-t0)/ITERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f5033",
   "metadata": {},
   "source": [
    "## JIT + space minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc03b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a6796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79357a05",
   "metadata": {},
   "source": [
    "## no JIT + space minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9be80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f5cd90d",
   "metadata": {},
   "source": [
    "# INFINITE HORIZON: ONLINE LEARNING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bf0c",
   "metadata": {},
   "source": [
    "## Validation for the model\n",
    "\n",
    "the Kalman filter usually has the following loop:\n",
    "\n",
    "- For i in infinity:\n",
    "    - Get values of t, R (just add one to t and maintain the same R)\n",
    "    - model.predict(X = t, R = R)\n",
    "    - model.some_update_fn(Y = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc0dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_test, var_test = model.predict_y(X = t_test[0:2], R = R_test[0:2])\n",
    "for i in range(2, 50, 2):\n",
    "    mean_test_i, var_test_i = model.predict_y(X = t_test[i:i+5], R = R_test[i:i+5])\n",
    "    mean_test = np.concatenate([mean_test, mean_test_i])\n",
    "    var_test = np.concatenate([var_test, var_test_i])\n",
    "    model.update_posterior()\n",
    "    model.update_variational_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1296b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_mean_ts, posterior_var_ts = model.predict_y(X=t, R=R_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6f995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d190e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.posterior_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693228f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = model.posterior_mean\n",
    "model.X = np.append(t_train, t_test[0])\n",
    "model.Y = np.append(Y_train, Y_test[0])\n",
    "model.R = np.append(R_train_scaled, R_test_scaled[0])\n",
    "model.update_posterior()\n",
    "b = model.posterior_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae831113",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a == b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c201e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.posterior_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_variational_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.filter(1, model.kernel, 0.5, model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6de79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_mean = model.compute_full_pseudo_lik()[0]\n",
    "latent_var = model.compute_full_pseudo_lik()[1]\n",
    "\n",
    "predictive_pseudo = model.likelihood.predict(latent_mean, latent_var, model.mask_pseudo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS METHOD GETS PREDICTIONS DONE \n",
    "\n",
    "# lik = bayesnewton.likelihoods.Gaussian(variance=VAR_Y)\n",
    "# inf = bayesnewton.inference.Taylor\n",
    "# mod = bayesnewton.basemodels.MarkovGP\n",
    "# Mod = bayesnewton.build_model(mod, inf)\n",
    "# model = Mod(kernel=kern, likelihood=lik, X=t_train, Y=Y_train, R=R_train_scaled)\n",
    "\n",
    "# model.inference()\n",
    "# peudo_y = model.compute_full_pseudo_lik()[0]\n",
    "# noise_cov = model.compute_full_pseudo_lik()[1]\n",
    "# model.update_posterior()\n",
    "# log_lik, (filter_mean, filter_cov) = model.filter(model.dt, model.kernel, peudo_y, noise_cov)\n",
    "\n",
    "# filter_mean[:,0,0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
