{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f385e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 13:39:01 prophet.plot ERROR: Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from statsforecast.adapters.prophet import AutoARIMAProphet\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sys = 10\n",
    "n_test = 24\n",
    "n_data = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5ef618",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime').drop(columns=['2657', '2828']) #DROPPING FAULTY SYSTEMS\n",
    "uk_pv = pd.read_csv('../../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "\n",
    "data_multiple = data.iloc[:, :n_sys][:n_data].reset_index()\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.columns)].set_index('ss_id_string')['kwp'].values * 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bd114",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "First I run the autoarima on a subsample of the models, see what model gets chosen most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4f7d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing time series for system 2607\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2625\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2626\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2631\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2660\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2729\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2760\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2766\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2770\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "constructing time series for system 2775\n",
      "defining the model\n",
      "fit the model\n",
      "save the data in a dict\n",
      "time it took is 1321.2330470085144\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "models = dict.fromkeys(data.columns[:n_sys])\n",
    "\n",
    "for idx_series, system_id in enumerate(data.columns[:n_sys]):\n",
    "    print(f'constructing time series for system {system_id}')\n",
    "    ts = data.iloc[:n_data, idx_series].to_frame()\n",
    "    ts = ts.T.stack().to_frame().reset_index().set_index('level_0')\n",
    "    ts = ts.rename(index={'level_0':'unique_id'}, columns = {'datetime':'ds', 0:'y'})\n",
    "    ts.index = ts.index.rename('unique_id')\n",
    "\n",
    "    #following this to put constant trend: https://github.com/facebook/prophet/issues/614\n",
    "    print(f'defining the model')\n",
    "    m = AutoARIMAProphet(growth = 'flat', yearly_seasonality = True, weekly_seasonality =False, daily_seasonality = True,\n",
    "                            seasonality_mode = 'multiplicative', \n",
    "                            d=1,\n",
    "                            D=1,\n",
    "                            max_p=2,\n",
    "                            max_q=2,\n",
    "                            max_P=2,\n",
    "                            max_Q=2,\n",
    "                            start_p=1,\n",
    "                            start_q=1,\n",
    "                            start_P=1,\n",
    "                            start_Q=1,\n",
    "                            stationary=False,\n",
    "                            seasonal=True,\n",
    "                            ic='aicc',\n",
    "                            stepwise=True,\n",
    "                            nmodels=40,\n",
    "                            test='kpss',\n",
    "                            seasonal_test='seas',\n",
    "                            num_cores=None,\n",
    "                            period=97,\n",
    "                            approximation = True,\n",
    "    )\n",
    "    print('fit the model')\n",
    "    m.fit(ts)\n",
    "    print('save the data in a dict')\n",
    "    models[system_id] = m.arima.model_\n",
    "    \n",
    "end =  time.time()\n",
    "time_loop = end - start\n",
    "print(f'time it took is {time_loop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6dd6eb",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Check which model is chosen most often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5c972b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2607': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2625': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2626': ARIMA(1,1,2)(0,1,0)[97]                   ,\n",
       " '2631': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2660': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2729': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2760': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2766': ARIMA(1,1,2)(0,1,0)[97]                   ,\n",
       " '2770': ARIMA(2,1,2)(0,1,0)[97]                   ,\n",
       " '2775': ARIMA(2,1,2)(0,1,0)[97]                   }"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40937d8",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "- Train the most popular model on the whole data for the first 2 years (this way we don't need to do autoarima)\n",
    "\n",
    "The most popular model was ARIMA(2,1,2)(0,1,0)[97] \n",
    "\n",
    "- Perform Cross Validation on these models, using the builtin function\n",
    "\n",
    "\n",
    "- Add also the features to convert the MAE into MW units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc0340d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing time series for system 2607\n",
      "defining the model\n",
      "fit the model\n",
      "Fitting models using approximations to speed things up\n",
      "\n",
      "ARIMA(2,1,2)(0,1,0)[97]                   :-14355.002561757196\n",
      "\n",
      "ARIMA(0,1,0)(0,1,0)[97]                   :-13555.17042403563\n",
      "\n",
      "ARIMA(1,1,0)(0,1,0)[97]                   :-13683.367419417567\n",
      "\n",
      "ARIMA(0,1,1)(0,1,0)[97]                   :-13767.806204148312\n",
      "\n",
      "ARIMA(1,1,2)(0,1,0)[97]                   :-14295.054262733587\n",
      "Now re-fitting the best model(s) without approximations...\n",
      "\n",
      "\n",
      "ARIMA(2,1,2)(0,1,0)[97]                   :-14867.90504023886\n",
      "save the data in a dict\n",
      "time it took is 148.4996371269226\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "models = dict.fromkeys(data.columns[:n_sys])\n",
    "models_params = dict.fromkeys(data.columns[:n_sys])\n",
    "# df_p_tot = dict.fromkeys(data.columns[:n_sys])\n",
    "# df_cv_tot = dict.fromkeys(data.columns[:n_sys])\n",
    "\n",
    "for idx_series, system_id in enumerate(data.columns[:n_sys]):\n",
    "    print(f'constructing time series for system {system_id}')\n",
    "    ts = data.iloc[:n_data, idx_series].to_frame()\n",
    "    ts = ts.T.stack().to_frame().reset_index().set_index('level_0')\n",
    "    ts = ts.rename(index={'level_0':'unique_id'}, columns = {'datetime':'ds', 0:'y'})\n",
    "    ts.index = ts.index.rename('unique_id')\n",
    "\n",
    "    #following this to put constant trend: https://github.com/facebook/prophet/issues/614\n",
    "    print(f'defining the model')\n",
    "    m = AutoARIMAProphet(growth = 'flat', yearly_seasonality = True, weekly_seasonality =False, daily_seasonality = True,\n",
    "                            seasonality_mode = 'multiplicative', \n",
    "                            d=1,\n",
    "                            D=1,\n",
    "                            max_p=2,\n",
    "                            max_q=2,\n",
    "                            max_P=0,\n",
    "                            max_Q=0,\n",
    "                            start_p=2,\n",
    "                            start_q=2,\n",
    "                            stationary=False,\n",
    "                            seasonal=True,\n",
    "                            ic='aicc',\n",
    "                            stepwise=True,\n",
    "                            nmodels=5,\n",
    "                            test='kpss',\n",
    "                            seasonal_test='seas',\n",
    "                            num_cores=None,\n",
    "                            period=97,\n",
    "                            approximation = True,\n",
    "                            trace=True                  \n",
    "    )\n",
    "    print('fit the model')\n",
    "    m.fit(ts)\n",
    "\n",
    "    print('save the data in a dict')\n",
    "    models[system_id] = m\n",
    "    models_params[system_id] = m.arima.model_\n",
    "\n",
    "end =  time.time()\n",
    "time_loop = end - start\n",
    "print(f'time it took is {time_loop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74f0eb",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "- Tesitng the model without retraining\n",
    "\n",
    "\n",
    "from: https://stackoverflow.com/questions/66870279/arima-forecasting-next-steps-without-updating-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = data.index \n",
    "data_index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336e06d",
   "metadata": {},
   "source": [
    "## Statsmodels ARIMA is too slow!\n",
    "Instead I will try to get the model with statsforecasts, and then pass it to statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54da203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys(d, keys):\n",
    "    return dict([(keys.get(k), v) for k, v in d.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91a5da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "M = models['2607']\n",
    "model_coeff = M.arima.model_.model['coef']\n",
    "model_coefficient = copy.deepcopy(model_coeff)\n",
    "mapping = {'ma1':'ma.L1', 'ma2':'ma.L2', 'ar1':'ar.L1', 'ar2':'ar.L2', 'sigma2':'sigma2'}\n",
    "model_coefficient = rename_keys(model_coefficient, mapping)\n",
    "\n",
    "#setting the sigma to 0\n",
    "model_coefficient['sigma2'] = M.arima.model_.model['sigma2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57459032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_coefficient['ar1'] = model_coefficient.pop('ar.L.L1')\n",
    "# model_coefficient['ar2'] = model_coefficient.pop('ar.L.L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88a9eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took is 42.88771462440491\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "start = time.time()\n",
    "series = data.iloc[:,0].reset_index().drop(columns='datetime')\n",
    "# Split data into test and training\n",
    "nobs = len(series)\n",
    "n_train = 10000\n",
    "series_train = series.iloc[:n_train]\n",
    "\n",
    "# Fit the model using the training dataset\n",
    "p, q, P, Q, per, d, D = [M.arima.model_.model['arma'][i]  for i in range(7)] #order=(2,1,2), seasonal_order=(0,1,0,97)\n",
    "\n",
    "#USING THE PARAMETERS FROM THE MODEL, AND FIXING THE INITIAL PARAMETERS\n",
    "model = ARIMA(series_train, order=(p,d,q), seasonal_order=(P,D,Q,per))\n",
    "with model.fix_params(model_coefficient):\n",
    "        fit = model.fit()\n",
    "\n",
    "end =  time.time()\n",
    "time_loop = end - start\n",
    "print(f'time it took is {time_loop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "903c6bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2607</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.324518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.333120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.379733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.321640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.277653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2607\n",
       "0     0.000000\n",
       "1     0.000000\n",
       "2     0.000000\n",
       "3     0.000000\n",
       "4     0.000000\n",
       "...        ...\n",
       "9995  0.324518\n",
       "9996  0.333120\n",
       "9997  0.379733\n",
       "9998  0.321640\n",
       "9999  0.277653\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d4c7a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2607</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.266683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.268018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           2607\n",
       "10000  0.266683\n",
       "10001  0.268018"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.iloc[t:t + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acd13c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.tsa.arima.model.ARIMAResultsWrapper at 0x1499cea00>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a41cf7db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing step 10000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoing step\u001b[39m\u001b[38;5;124m'\u001b[39m,t)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Update the results by appending the next observation\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Produce a forecast for t+1 based on data through t\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforecasting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/arima/model.py:511\u001b[0m, in \u001b[0;36mARIMAResults.append\u001b[0;34m(self, endog, exog, refit, fit_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mexog_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_input_exog_names\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Perform the appending procedure\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# Now we reverse the temporary change made above\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:4063\u001b[0m, in \u001b[0;36mMLEResults.append\u001b[0;34m(self, endog, exog, refit, fit_kwargs, copy_initialization, **kwargs)\u001b[0m\n\u001b[1;32m   4060\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialization\u001b[39m\u001b[38;5;124m'\u001b[39m, init)\n\u001b[1;32m   4062\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclone(new_endog, exog\u001b[38;5;241m=\u001b[39mnew_exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 4063\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:3667\u001b[0m, in \u001b[0;36mMLEResults._apply\u001b[0;34m(self, mod, refit, fit_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   3665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mfix_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fixed_params):\n\u001b[1;32m   3666\u001b[0m         fit_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincludes_fixed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 3667\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3668\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:889\u001b[0m, in \u001b[0;36mMLEModel.smooth\u001b[0;34m(self, params, transformed, includes_fixed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm\u001b[38;5;241m.\u001b[39msmooth(complex_step\u001b[38;5;241m=\u001b[39mcomplex_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Wrap in a results object\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_ssm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mresults_wrapper_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:788\u001b[0m, in \u001b[0;36mMLEModel._wrap_results\u001b[0;34m(self, params, result, return_raw, cov_type, cov_kwds, results_class, wrapper_class)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrapper_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m         wrapper_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_res_classes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 788\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mresults_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrapper_class(res)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:1806\u001b[0m, in \u001b[0;36mSARIMAXResults.__init__\u001b[0;34m(self, model, params, filter_results, cov_type, **kwargs)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, params, filter_results, cov_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1805\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1806\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSARIMAXResults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_resid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf  \u001b[38;5;66;03m# attribute required for wald tests\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# Save _init_kwds\u001b[39;00m\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:2337\u001b[0m, in \u001b[0;36mMLEResults.__init__\u001b[0;34m(self, model, params, results, cov_type, cov_kwds, **kwargs)\u001b[0m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_robustcov_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_self\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcov_kwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m   2340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:2553\u001b[0m, in \u001b[0;36mMLEResults._get_robustcov_results\u001b[0;34m(self, cov_type, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2552\u001b[0m         mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39ms_[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m-> 2553\u001b[0m     res\u001b[38;5;241m.\u001b[39m_rank \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_params_default\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cov_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   2555\u001b[0m     res\u001b[38;5;241m.\u001b[39mcov_params_default \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((k_params, k_params)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/numpy/linalg/linalg.py:1891\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(A, tol, hermitian)\u001b[0m\n\u001b[1;32m   1889\u001b[0m S \u001b[38;5;241m=\u001b[39m svd(A, compute_uv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, hermitian\u001b[38;5;241m=\u001b[39mhermitian)\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1891\u001b[0m     tol \u001b[38;5;241m=\u001b[39m \u001b[43mS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;241m*\u001b[39m finfo(S\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1893\u001b[0m     tol \u001b[38;5;241m=\u001b[39m asarray(tol)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, newaxis]\n",
      "File \u001b[0;32m~/prophet/lib/python3.8/site-packages/numpy/core/_methods.py:40\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Compute the first forecast based only on the training dataset\n",
    "forecasts = []\n",
    "res = fit\n",
    "forecasts.append(res.forecast(12))\n",
    "\n",
    "# Now step through the test observations:\n",
    "# (a) add the new observation without refitting the model\n",
    "# (b) produce a new forecast\n",
    "for t in range(n_train, n_train+20):#nobs):\n",
    "    \n",
    "    # REMEMBER TO NOT FORECAST ON THE FIRST 12 TIMESTEPS, AND ON THE NEXT DAY WORTH OF DATA\n",
    "    print('doing step',t)\n",
    "    # Update the results by appending the next observation\n",
    "    res = res.append(series.iloc[t:t + 2], refit=False)\n",
    "    # Produce a forecast for t+1 based on data through t\n",
    "    print('forecasting')\n",
    "    forecasts.append(res.forecast(12))\n",
    "    print('performed forecasts')\n",
    "    \n",
    "end =  time.time()\n",
    "time_loop = end - start\n",
    "print(f'time it took is {time_loop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/44559180/2901002\n",
    "def justify(a, invalid_val=0, axis=1, side='left'):    \n",
    "    \"\"\"\n",
    "    Justifies a 2D array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : ndarray\n",
    "        Input array to be justified\n",
    "    axis : int\n",
    "        Axis along which justification is to be made\n",
    "    side : str\n",
    "        Direction of justification. It could be 'left', 'right', 'up', 'down'\n",
    "        It should be 'left' or 'right' for axis=1 and 'up' or 'down' for axis=0.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if invalid_val is np.nan:\n",
    "        mask = ~np.isnan(a)\n",
    "    else:\n",
    "        mask = a!=invalid_val\n",
    "    justified_mask = np.sort(mask,axis=axis)\n",
    "    if (side=='up') | (side=='left'):\n",
    "        justified_mask = np.flip(justified_mask,axis=axis)\n",
    "    out = np.full(a.shape, invalid_val) \n",
    "    if axis==1:\n",
    "        out[justified_mask] = a[mask]\n",
    "    else:\n",
    "        out.T[justified_mask.T] = a.T[mask.T]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE CREATED A DATASET OF OBSERVATIONS OVER TIME, compresses it, and calculates the error as function of horizon\n",
    "df_preds = pd.concat(forecasts,axis=1)\n",
    "arr = justify(df_preds.T.to_numpy(), invalid_val=np.nan)\n",
    "df = pd.DataFrame(arr).dropna(axis=1, how='all').T\n",
    "forecast_error = (df.T - series[100:121].values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d0b3f",
   "metadata": {},
   "source": [
    "## Step X\n",
    "- Once I work on the cross validation for the Gaussian Process, use the same methodology for this ARIMA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# models = dict.fromkeys(data.columns[:n_sys])\n",
    "# models_params = dict.fromkeys(data.columns[:n_sys])\n",
    "# preds = dict.fromkeys(data.columns[:n_sys])\n",
    "\n",
    "# for idx_series, system_id in enumerate(data.columns[:n_sys]):\n",
    "#     print(f'constructing time series for system {system_id}')\n",
    "#     ts_test = data.iloc[n_data:, idx_series].to_frame()\n",
    "#     ts = ts_test.T.stack().to_frame().reset_index().set_index('level_0')\n",
    "#     ts_test = ts_test.rename(index={'level_0':'unique_id'}, columns = {'datetime':'ds', 0:'y'})\n",
    "#     ts_test.index = ts_test.index.rename('unique_id')\n",
    "\n",
    "#     model_fit = models[system_id]\n",
    "    \n",
    "#     print('create the future df')\n",
    "#     day_max = 16\n",
    "#     day_min = 8\n",
    "#     M = models['2607']\n",
    "#     future = M.make_future_dataframe(periods=1000, freq = '5min', include_history=False)\n",
    "#     future2 = future[(future.ds.dt.hour >= 8)  & (future.ds.dt.hour < 16) ]\n",
    "#     future2 = future2.iloc[:n_test]\n",
    "#     print('forecast')\n",
    "#     forecast = M.predict(future2).dropna(subset='ds')\n",
    "    \n",
    "    \n",
    "# end =  time.time()\n",
    "# time_loop = end - start\n",
    "# print(f'time it took is {time_loop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print('create the future df')\n",
    "#     day_max = 16\n",
    "#     day_min = 8\n",
    "#     future = m.make_future_dataframe(periods=1000, freq = '5min')\n",
    "#     future2 = future[(future.ds.dt.hour >= 8)  & (future.ds.dt.hour < 16) ]\n",
    "#     future2 = future2.iloc[:len(ts) + n_test]\n",
    "\n",
    "#     print('forecast')\n",
    "#     forecast = m.predict(future2).dropna(subset='ds')\n",
    "    \n",
    "#     print('perform CV')\n",
    "#     init = str(int((n_data/97) / 2)) + ' days' #start midway trough the ts\n",
    "# #     init = '400 days' #give enough time to have a full year of data for seasonality\n",
    "#     period = str(max(1, int((n_data/97) / 2 / 35))) + ' days ,  2 hours' #divide into 50 predictions, add 2 hours\n",
    "#     df_cv = cross_validation(m, initial=init, period=period, horizon = '2 hours')\n",
    "#     df_p = performance_metrics(df_cv)\n",
    "\n",
    "\n",
    "#     df_p_tot[system_id] = df_p\n",
    "#     df_cv_tot[system_id] = df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_df = pd.concat(df_p_tot)\n",
    "# mean_accuracy_result = accuracy_df.droplevel(1).groupby(axis=0, level=0).mean() \n",
    "# models_df = pd.DataFrame.from_dict(models, orient='index')\n",
    "# maes = (mean_accuracy_result.T * capacities).loc['mae']\n",
    "\n",
    "\n",
    "# mean_accuracy_result.to_csv('accuracies')\n",
    "# models_df.to_csv('models')\n",
    "# maes.to_csv('maes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95057d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prophet.plot import plot_cross_validation_metric\n",
    "# fig = plot_cross_validation_metric(df_cv, metric='mae')\n",
    "\n",
    "# fig1 = m.plot(forecast,)\n",
    "# plt.xlim(forecast.ds[800], forecast.ds[1023])\n",
    "# m.arima.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
