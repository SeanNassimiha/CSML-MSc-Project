{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc3959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda7e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_differenced(periods, df_original, df_differences):\n",
    "    '''\n",
    "    from: https://stackoverflow.com/questions/34918013/undo-a-series-diff\n",
    "    '''\n",
    "    restored = df_original.copy()\n",
    "    restored.iloc[periods:] = np.nan\n",
    "    for d, val in df_differences.iloc[periods:].iterrows():\n",
    "        restored.iloc[d] = restored.iloc[d - periods] + val\n",
    "    return restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee52329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_score_function(actual, predicted, var):\n",
    "    '''\n",
    "    Function that calculates the log scoring function:\n",
    "    \n",
    "    L = 0.5 * sum_j (ln(var_j) + (y - y_hat)^2 / var_j)\n",
    "    \n",
    "    '''\n",
    "    var[var == 0] = np.nan\n",
    "    L = 0.5 * np.nansum(np.log(var) + (actual - predicted)**2 / var, axis=1)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b565a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sys_VAR = 400\n",
    "n_data_VAR = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289e6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../../../Data/pv_power_df_5day_capacity_scaled.csv', index_col='datetime').drop(columns=['2657', '2828']) #DROPPING FAULTY SYSTEMS\n",
    "uk_pv = pd.read_csv('../../../Data/system_metadata_location_rounded.csv')\n",
    "uk_pv['ss_id_string'] = uk_pv['ss_id'].astype('str')\n",
    "data_multiple = data.iloc[:, :n_sys_VAR][:n_data_VAR]\n",
    "capacities = uk_pv[uk_pv.ss_id_string.isin(data_multiple.reset_index().columns)].set_index('ss_id_string')['kwp'].values * 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215266b3",
   "metadata": {},
   "source": [
    "# Do the same for other benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37fa96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for persistence\n",
      "persistence calculation time: 0.00 secs\n",
      "persistence calculation time: 0.00 secs\n",
      "persistence calculation time: 0.00 secs\n",
      "persistence calculation time: 0.00 secs\n",
      "persistence calculation time: 0.00 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.01 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "persistence calculation time: 0.02 secs\n",
      "Getting results for yesterday\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.00 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.01 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "yesterday calculation time: 0.02 secs\n",
      "Getting results for hourly_average\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.00 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.01 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.02 secs\n",
      "hourly_average calculation time: 0.03 secs\n",
      "hourly_average calculation time: 0.03 secs\n",
      "hourly_average calculation time: 0.03 secs\n",
      "hourly_average calculation time: 0.03 secs\n",
      "hourly_average calculation time: 0.03 secs\n",
      "Getting results for VAR\n",
      "VAR calculation time: 0.40 secs\n",
      "VAR calculation time: 0.82 secs\n",
      "VAR calculation time: 1.24 secs\n",
      "VAR calculation time: 1.63 secs\n",
      "VAR calculation time: 2.06 secs\n",
      "VAR calculation time: 2.51 secs\n",
      "VAR calculation time: 2.98 secs\n",
      "VAR calculation time: 3.47 secs\n",
      "VAR calculation time: 3.90 secs\n",
      "VAR calculation time: 4.37 secs\n",
      "VAR calculation time: 4.82 secs\n",
      "VAR calculation time: 5.50 secs\n",
      "VAR calculation time: 5.95 secs\n",
      "VAR calculation time: 6.31 secs\n",
      "VAR calculation time: 6.84 secs\n",
      "VAR calculation time: 7.29 secs\n",
      "VAR calculation time: 7.74 secs\n",
      "VAR calculation time: 8.09 secs\n",
      "VAR calculation time: 8.54 secs\n",
      "VAR calculation time: 8.87 secs\n",
      "VAR calculation time: 9.25 secs\n",
      "VAR calculation time: 9.58 secs\n",
      "VAR calculation time: 10.04 secs\n",
      "VAR calculation time: 10.47 secs\n",
      "VAR calculation time: 10.91 secs\n",
      "VAR calculation time: 11.37 secs\n",
      "VAR calculation time: 11.79 secs\n",
      "VAR calculation time: 12.11 secs\n",
      "VAR calculation time: 12.55 secs\n",
      "VAR calculation time: 13.00 secs\n",
      "VAR calculation time: 13.36 secs\n",
      "VAR calculation time: 13.78 secs\n",
      "VAR calculation time: 14.13 secs\n",
      "VAR calculation time: 14.45 secs\n",
      "VAR calculation time: 14.80 secs\n",
      "VAR calculation time: 15.11 secs\n",
      "VAR calculation time: 15.41 secs\n",
      "VAR calculation time: 15.75 secs\n",
      "VAR calculation time: 16.11 secs\n",
      "VAR calculation time: 16.41 secs\n",
      "Getting results for SimpleExpSmoothing\n",
      "SimpleExpSmoothing calculation time: 1.34 secs\n",
      "SimpleExpSmoothing calculation time: 2.69 secs\n",
      "SimpleExpSmoothing calculation time: 4.07 secs\n",
      "SimpleExpSmoothing calculation time: 5.39 secs\n",
      "SimpleExpSmoothing calculation time: 6.79 secs\n",
      "SimpleExpSmoothing calculation time: 8.18 secs\n",
      "SimpleExpSmoothing calculation time: 9.56 secs\n",
      "SimpleExpSmoothing calculation time: 10.96 secs\n",
      "SimpleExpSmoothing calculation time: 12.40 secs\n",
      "SimpleExpSmoothing calculation time: 13.88 secs\n",
      "SimpleExpSmoothing calculation time: 15.36 secs\n",
      "SimpleExpSmoothing calculation time: 16.85 secs\n",
      "SimpleExpSmoothing calculation time: 18.27 secs\n",
      "SimpleExpSmoothing calculation time: 19.97 secs\n",
      "SimpleExpSmoothing calculation time: 21.52 secs\n",
      "SimpleExpSmoothing calculation time: 22.89 secs\n",
      "SimpleExpSmoothing calculation time: 24.39 secs\n",
      "SimpleExpSmoothing calculation time: 25.85 secs\n",
      "SimpleExpSmoothing calculation time: 27.24 secs\n",
      "SimpleExpSmoothing calculation time: 28.76 secs\n",
      "SimpleExpSmoothing calculation time: 30.14 secs\n",
      "SimpleExpSmoothing calculation time: 31.48 secs\n",
      "SimpleExpSmoothing calculation time: 32.84 secs\n",
      "SimpleExpSmoothing calculation time: 34.18 secs\n",
      "SimpleExpSmoothing calculation time: 35.53 secs\n",
      "SimpleExpSmoothing calculation time: 36.83 secs\n",
      "SimpleExpSmoothing calculation time: 38.22 secs\n",
      "SimpleExpSmoothing calculation time: 39.56 secs\n",
      "SimpleExpSmoothing calculation time: 40.88 secs\n",
      "SimpleExpSmoothing calculation time: 42.26 secs\n",
      "SimpleExpSmoothing calculation time: 43.66 secs\n",
      "SimpleExpSmoothing calculation time: 45.02 secs\n",
      "SimpleExpSmoothing calculation time: 46.35 secs\n",
      "SimpleExpSmoothing calculation time: 47.73 secs\n",
      "SimpleExpSmoothing calculation time: 49.18 secs\n",
      "SimpleExpSmoothing calculation time: 50.61 secs\n",
      "SimpleExpSmoothing calculation time: 51.99 secs\n",
      "SimpleExpSmoothing calculation time: 53.52 secs\n",
      "SimpleExpSmoothing calculation time: 55.20 secs\n",
      "SimpleExpSmoothing calculation time: 56.77 secs\n",
      "Getting results for ExponentialSmoothing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExponentialSmoothing calculation time: 55.56 secs\n",
      "ExponentialSmoothing calculation time: 120.29 secs\n",
      "ExponentialSmoothing calculation time: 185.80 secs\n",
      "ExponentialSmoothing calculation time: 287.42 secs\n",
      "ExponentialSmoothing calculation time: 356.38 secs\n",
      "ExponentialSmoothing calculation time: 430.20 secs\n"
     ]
    }
   ],
   "source": [
    "models_list = ['persistence','yesterday','hourly_average' , 'VAR', 'SimpleExpSmoothing', 'ExponentialSmoothing',]\n",
    "\n",
    "MAE_results = dict.fromkeys(models_list)\n",
    "NLPD_results = dict.fromkeys(models_list)\n",
    "\n",
    "for model_type in models_list:\n",
    "        \n",
    "    t1 = time.time()\n",
    "    print(f'Getting results for {model_type}')\n",
    "    #FIXED WINDOW OF 5000 train and 24 test, the 5000 train slide forward\n",
    "    length_window = 97 * 10\n",
    "    max_t = len(data_multiple) - length_window - 24\n",
    "    errors = np.zeros((24, n_sys_VAR))\n",
    "\n",
    "    for t in range(10000, max_t, 1000):\n",
    "        data_multiple_iter = data_multiple.iloc[t:t+length_window + 24]\n",
    "        data_multiple_train = data_multiple_iter.iloc[:length_window] \n",
    "        data_multiple_test = data_multiple_iter[length_window:]  \n",
    "\n",
    "        if model_type == 'VAR':\n",
    "\n",
    "            data_VAR = data_multiple_train.diff().diff(97).dropna()\n",
    "\n",
    "            #CREATE MODEL AND PREDICT NEXT 24\n",
    "            model = VAR(data_VAR)\n",
    "            model_fit = model.fit()\n",
    "            lag_order = model_fit.k_ar\n",
    "            preds = model_fit.forecast(data_VAR.values[-lag_order:], 24)\n",
    "            if len(preds[preds>10]) > 0:\n",
    "                print('Careful, a prediction is higher than 10!')\n",
    "            #evaluate forecast\n",
    "            df_forecast = pd.DataFrame(preds, index=data_multiple_test.index, columns=data_VAR.columns)\n",
    "\n",
    "            data_total = pd.concat([data_VAR,df_forecast], axis=0).reindex(data_multiple_iter.index).reset_index().drop(columns = ['datetime'])\n",
    "\n",
    "            data_reset = data_total.iloc[1:].reset_index().drop(columns=['index'])\n",
    "            restored = restore_differenced(97, data_multiple_iter.diff().dropna(), data_reset)\n",
    "            restored = restored.reindex(data_multiple_iter.index).reset_index().drop(columns=['datetime'])\n",
    "            restored_twice = restore_differenced(1, data_multiple_iter, restored)\n",
    "\n",
    "            #CLIPPING PREDICTIONS BETWEEN 0 AND 1\n",
    "            restored_twice = restored_twice.clip(0,1)\n",
    "            predictions = restored_twice.iloc[-24:]    \n",
    "\n",
    "        elif model_type == 'persistence':\n",
    "            predictions = data_multiple_train.iloc[-1].values\n",
    "\n",
    "        elif model_type == 'yesterday':\n",
    "            predictions = np.zeros((1,n_sys_VAR))\n",
    "            previous_day = data_multiple_train.iloc[-97:].values\n",
    "            for i in range(24):\n",
    "                pred = previous_day[-97 + i][np.newaxis, :]\n",
    "                predictions = np.concatenate((predictions, pred))\n",
    "            predictions = predictions[1:]\n",
    "\n",
    "        elif model_type == 'hourly_average':\n",
    "            predictions = np.zeros((1,n_sys_VAR))\n",
    "            previous_hour = data_multiple_train.iloc[-12:].values\n",
    "            for i in range(24):\n",
    "                pred = previous_hour.mean(axis=0)[np.newaxis, :]\n",
    "                predictions = np.concatenate((predictions, pred))\n",
    "                #HERE I append the latest prediction and remove the oldest observation\n",
    "                previous_hour = np.concatenate((previous_hour, pred), axis=0)[1:]\n",
    "            predictions = predictions[1:]\n",
    "            \n",
    "            \n",
    "        elif model_type == 'SimpleExpSmoothing':\n",
    "            predictions = np.zeros((24, 1))\n",
    "            variances = np.zeros((24, 1))\n",
    "            for ts in range(data_multiple_train.shape[1]):\n",
    "                model = SimpleExpSmoothing(data_multiple_train.iloc[:,ts], initialization_method=\"estimated\")\n",
    "                model_fit = model.fit()\n",
    "                \n",
    "                fcast = model_fit.forecast(24).values[:, np.newaxis]\n",
    "                predictions = np.concatenate((predictions, fcast), axis=1)\n",
    "                \n",
    "                var = model_fit.simulate(nsimulations = 24, anchor = 'end', repetitions = 1000 ).var(axis=1).values[:, np.newaxis]\n",
    "                \n",
    "                var_low_bound = (fcast**2 / 4)\n",
    "                var_upper_bound = ((1-fcast)**2 / 4)\n",
    "                var = np.maximum(var_low_bound, var)\n",
    "                var = np.minimum(var_upper_bound, var)\n",
    "                \n",
    "                variances = np.concatenate((variances, var), axis=1)\n",
    "                \n",
    "                \n",
    "            predictions = predictions[:, 1:]\n",
    "            variances = variances[:, 1:]\n",
    "            \n",
    "            nlpd = log_score_function(data_multiple_iter.iloc[-24:].values, predictions, variances) \n",
    "            \n",
    "            \n",
    "        elif model_type == 'ExponentialSmoothing':\n",
    "            predictions = np.zeros((24, 1))\n",
    "            variances = np.zeros((24, 1))\n",
    "            for ts in range(data_multiple_train.shape[1]):\n",
    "                model = ExponentialSmoothing(data_multiple_train.iloc[:,ts], \n",
    "                                             seasonal_periods=97,\n",
    "                                             seasonal=\"add\",\n",
    "                                             initialization_method=\"estimated\")\n",
    "                model_fit = model.fit()\n",
    "                fcast = model_fit.forecast(24).values[:, np.newaxis]\n",
    "                predictions = np.concatenate((predictions, fcast), axis=1)\n",
    "                \n",
    "                var = model_fit.simulate(nsimulations = 24, anchor = 'end', repetitions = 1000 ).var(axis=1).values[:, np.newaxis]\n",
    "                \n",
    "                var_low_bound = (fcast**2 / 4)\n",
    "                var_upper_bound = ((1-fcast)**2 / 4)\n",
    "                var = np.maximum(var_low_bound, var)\n",
    "                var = np.minimum(var_upper_bound, var)\n",
    "                \n",
    "                variances = np.concatenate((variances, var), axis=1)\n",
    "                \n",
    "            predictions = predictions[:, 1:]\n",
    "            variances = variances[:, 1:]\n",
    "            \n",
    "            nlpd = log_score_function(data_multiple_iter.iloc[-24:].values, predictions, variances) \n",
    "\n",
    "            \n",
    "        #THIS WAY WE REDUCE THE ERROR FURTHER USING A SIMPLE TRICK OF CLIPPING PREDICTIONS OUTSIDE DOMAIN\n",
    "        predictions = predictions.clip(0,1)\n",
    "        #Get error\n",
    "        error = abs((predictions - data_multiple_iter.iloc[-24:]).values)\n",
    "        errors = np.concatenate((errors, error))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        print(f'{model_type} calculation time: %2.2f secs' % (t2-t1))\n",
    "\n",
    "    errors = errors.reshape(-1, 24, n_sys_VAR)[1:]\n",
    "    MAE_hsteps = np.mean(np.mean(errors, axis=0), axis=1)    \n",
    "    MAE_results[model_type] = MAE_hsteps\n",
    "    \n",
    "    if (model_type == 'SimpleExpSmoothing') or (model_type == 'ExponentialSmoothing'):\n",
    "        NLPD_results[model_type] = nlpd\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbaf1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NLPD_df = pd.DataFrame.from_dict(NLPD_results)\n",
    "NLPD_df.plot()\n",
    "plt.figure(figsize=(12,8))\n",
    "NLPD_df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_df = pd.DataFrame.from_dict(MAE_results)\n",
    "MAE_df.plot()\n",
    "plt.figure(figsize=(12,8))\n",
    "MAE_df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15e30c",
   "metadata": {},
   "source": [
    "## Uncertainty Intervals\n",
    "\n",
    "By using a state space formulation, we can perform simulations of future values. The mathematical details are described in Hyndman and Athanasopoulos [2] and in the documentation of HoltWintersResults.simulate.\n",
    "\n",
    "Hyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice, 2nd edition. OTexts, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86abb20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model_fit.forecast(24).values.clip(0,1)\n",
    "two_std = (model_fit.simulate(nsimulations = 24, anchor = 'end', repetitions = 1000 ).std(axis=1) * 2).values\n",
    "var = model_fit.simulate(nsimulations = 24, anchor = 'end', repetitions = 1000 ).var(axis=1).values\n",
    "upper_conf = (pred + two_std).clip(0,1)\n",
    "lower_conf = (pred - two_std).clip(0,1)\n",
    "\n",
    "plt.plot(pred)\n",
    "plt.plot(lower_conf)\n",
    "plt.plot(upper_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd416a4",
   "metadata": {},
   "source": [
    "# INCREASE THE WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = ['persistence','yesterday','hourly_average' , 'VAR', 'SimpleExpSmoothing', 'ExponentialSmoothing',]\n",
    "\n",
    "MAE_results = dict.fromkeys(models_list)\n",
    "\n",
    "for model_type in models_list:\n",
    "    print(f'Getting results for {model_type}')\n",
    "    #FIXED WINDOW OF 5000 train and 24 test, the 5000 train slide forward\n",
    "    length_window = 97 * 30\n",
    "    max_t = len(data_multiple) - length_window - 24\n",
    "    errors = np.zeros((24, n_sys_VAR))\n",
    "\n",
    "    for t in range(0, max_t, 5000):\n",
    "        data_multiple_iter = data_multiple.iloc[t:t+length_window + 24]\n",
    "        data_multiple_train = data_multiple_iter.iloc[:length_window] \n",
    "        data_multiple_test = data_multiple_iter[length_window:]  \n",
    "\n",
    "        if model_type == 'VAR':\n",
    "\n",
    "            data_VAR = data_multiple_train.diff().diff(97).dropna()\n",
    "\n",
    "            #CREATE MODEL AND PREDICT NEXT 24\n",
    "            model = VAR(data_VAR)\n",
    "            model_fit = model.fit()\n",
    "            lag_order = model_fit.k_ar\n",
    "            preds = model_fit.forecast(data_VAR.values[-lag_order:], 24)\n",
    "            if len(preds[preds>10]) > 0:\n",
    "                print('Careful, a prediction is higher than 10!')\n",
    "            #evaluate forecast\n",
    "            df_forecast = pd.DataFrame(preds, index=data_multiple_test.index, columns=data_VAR.columns)\n",
    "\n",
    "            data_total = pd.concat([data_VAR,df_forecast], axis=0).reindex(data_multiple_iter.index).reset_index().drop(columns = ['datetime'])\n",
    "\n",
    "            data_reset = data_total.iloc[1:].reset_index().drop(columns=['index'])\n",
    "            restored = restore_differenced(97, data_multiple_iter.diff().dropna(), data_reset)\n",
    "            restored = restored.reindex(data_multiple_iter.index).reset_index().drop(columns=['datetime'])\n",
    "            restored_twice = restore_differenced(1, data_multiple_iter, restored)\n",
    "\n",
    "            #CLIPPING PREDICTIONS BETWEEN 0 AND 1\n",
    "            restored_twice = restored_twice.clip(0,1)\n",
    "            predictions = restored_twice.iloc[-24:]    \n",
    "\n",
    "        elif model_type == 'persistence':\n",
    "            predictions = data_multiple_train.iloc[-1].values\n",
    "\n",
    "        elif model_type == 'yesterday':\n",
    "            predictions = np.zeros((1,n_sys_VAR))\n",
    "            previous_day = data_multiple_train.iloc[-97:].values\n",
    "            for i in range(24):\n",
    "                pred = previous_day[-97 + i][np.newaxis, :]\n",
    "                predictions = np.concatenate((predictions, pred))\n",
    "            predictions = predictions[1:]\n",
    "\n",
    "        elif model_type == 'hourly_average':\n",
    "            predictions = np.zeros((1,n_sys_VAR))\n",
    "            previous_hour = data_multiple_train.iloc[-12:].values\n",
    "            for i in range(24):\n",
    "                pred = previous_hour.mean(axis=0)[np.newaxis, :]\n",
    "                predictions = np.concatenate((predictions, pred))\n",
    "                #HERE I append the latest prediction and remove the oldest observation\n",
    "                previous_hour = np.concatenate((previous_hour, pred), axis=0)[1:]\n",
    "            predictions = predictions[1:]\n",
    "            \n",
    "        elif model_type == 'SimpleExpSmoothing':\n",
    "            predictions = np.zeros((24, 1))\n",
    "            for ts in range(data_multiple_train.shape[1]):\n",
    "                model = SimpleExpSmoothing(data_multiple_train.iloc[:,ts], initialization_method=\"estimated\")\n",
    "                model_fit = model.fit()\n",
    "                fcast = model_fit.forecast(24).values[:, np.newaxis]\n",
    "                predictions = np.concatenate((predictions, fcast), axis=1)\n",
    "            predictions = predictions[:, 1:]\n",
    "            \n",
    "        elif model_type == 'ExponentialSmoothing':\n",
    "            predictions = np.zeros((24, 1))\n",
    "            for ts in range(data_multiple_train.shape[1]):\n",
    "                model = ExponentialSmoothing(data_multiple_train.iloc[:,ts], \n",
    "                                             seasonal_periods=97,\n",
    "                                             seasonal=\"add\",\n",
    "                                             initialization_method=\"estimated\")\n",
    "                model_fit = model.fit()\n",
    "                fcast = model_fit.forecast(24).values[:, np.newaxis]\n",
    "                predictions = np.concatenate((predictions, fcast), axis=1)\n",
    "            predictions = predictions[:, 1:]\n",
    "\n",
    "        #Get error\n",
    "        error = abs((predictions - data_multiple_iter.iloc[-24:]).values)\n",
    "        errors = np.concatenate((errors, error))\n",
    "\n",
    "    errors = errors.reshape(-1, 24, n_sys_VAR)[1:]\n",
    "    MAE_hsteps = np.mean(np.mean(errors, axis=0)* capacities, axis=1)    \n",
    "    MAE_results[model_type] = MAE_hsteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_df = pd.DataFrame.from_dict(MAE_results)\n",
    "MAE_df.plot()\n",
    "plt.figure(figsize=(12,8))\n",
    "MAE_df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c57672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
